{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiSi90gspEQP"
   },
   "source": [
    "# Easy GPT-Q + LoRA in JAX ([github](https://github.com/davisyoshida/easy-lora-and-gptq))\n",
    "\n",
    "[Davis Yoshida](https://github.com/davisyoshida/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfxALa1so2JD"
   },
   "source": [
    "This notebook shows how to combine  two JAX tools/transforms I wrote: [Lorax](https://github.com/davisyoshida/lorax) and [JAX-GPTQ](https://github.com/davisyoshida/jax-gptq). I've been using the combination to run LLaMA finetunes on a single GPU.\n",
    "\n",
    "They're both applicable to basically any JAX function, which conveniently includes many HuggingFace models!\n",
    "\n",
    "The procedure is as follows:\n",
    "\n",
    "1. Quantize the weights of the model we want to use\n",
    "2. Use Lorax to transform the original model function `F(params, inputs)` to one that takes a tuple of the original params and the low rank LoRA params: `F_lora(param_tuple, inputs)`\n",
    "3. Wrap `F_lora` in `use_quantized` transform so that it knows how to handle arguments which are int8 matrices with two parameters per byte.\n",
    "4. Train the model, updating only the low rank params and leaving the larger 4-bit model weights frozen.\n",
    "\n",
    "I'd love feedback on one or both of these tools so please let me know on their Githubs if you have any suggestions. JAX-GPTQ in particular is still in a really early state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y6JeyF45yd_"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "75-T_R0Ms9qD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "# 아래 코드는 원하는 GPU 번호만 쓰도록 설정하는 코드\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "import lorax\n",
    "import jax_gptq\n",
    "\n",
    "gpu = jax.devices('gpu')[0]\n",
    "cpu = jax.devices('cpu')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQuDSjz7svdL"
   },
   "source": [
    "## Toy Example\n",
    "\n",
    "### Model/Data setup\n",
    "\n",
    "First we'll define an MLP and make some parameters for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Djyo_reAs26R"
   },
   "outputs": [],
   "source": [
    "N_LAYER = 5\n",
    "batch_size = 16\n",
    "DIM = 256\n",
    "\n",
    "def my_model(params, x):\n",
    "  for layer in params:\n",
    "    x = jax.nn.relu(x @ layer['w'] + layer['b'])\n",
    "\n",
    "  return jnp.mean(x)\n",
    "\n",
    "w_key, b_key, data_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "\n",
    "w_keys = jax.random.split(w_key, N_LAYER)\n",
    "b_keys = jax.random.split(b_key, N_LAYER)\n",
    "\n",
    "# Make some params\n",
    "params = [\n",
    "    {\n",
    "        'w': jax.random.normal(k1, (DIM, DIM)),\n",
    "        'b': jax.random.normal(k2, (DIM,))\n",
    "    }\n",
    "    for k1, k2 in zip(w_keys, b_keys)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlCLAmjBvhnA"
   },
   "source": [
    "GPT-Q needs input data for quantization. For an actual model we'd use real data but here we'll just make some random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6govTMOZvgSC"
   },
   "outputs": [],
   "source": [
    "quant_data = [jax.random.normal(key, (batch_size, DIM)) for key in jax.random.split(data_key, 16)]\n",
    "\n",
    "# We'll save an output for later comparison since the quantization process will delete the original params\n",
    "original_output = my_model(params, quant_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjdb3h46vtsi"
   },
   "source": [
    "### Run GPT-Q to get the quantized weights\n",
    "That's all for the setup, we can now just run GPT-Q (without any changes to the original model code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "L1Mw9ZLpvrLa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 0.00e+00 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 2it [00:20, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 3it [00:21,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 4it [00:23,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 5it [00:24,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 6it [00:26,  4.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Note that this may free the buffers associated with some or all of the parameters and the data to save VRAM\n",
    "# I'd also recommend you put the params on the CPU, since `quantize()` will move the params to th GPU when necessary\n",
    "quantized_params = jax_gptq.quantize(my_model, params, quant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NhVv8egwDQu"
   },
   "source": [
    "The matrices have been quantized but the biases have been left alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bWwXzTJyubbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W type: <class 'jax_gptq.gptq.QuantizedMatrix'>\n",
      "B type: <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(f'W type: {type(quantized_params[0][\"w\"])}')\n",
    "print(f'B type: {type(quantized_params[0][\"b\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwYLTr6WwapB"
   },
   "source": [
    "**Note**: The quantization procedure depends on the parameter being used in a matrix multiplication. Currently JAX-GPTQ supports general dot operations (including ones using tensors with any number of dimensions larger than 1), and convolutions with kernels of spatial size 1.\n",
    "\n",
    "### Applying the quantized weights\n",
    "We can now run the quantized model without any code changes. All that's necessary is using `jax_gptq.use_quantized` to transform the function so it knows how to handle `QuantizedMatrix` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I6aLdXqawQFs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of quantized network: 8.691e+04\n",
      "Original output: 8.575e+04\n"
     ]
    }
   ],
   "source": [
    "quantized_params = jax.device_put(quantized_params, gpu) # Move the params to the GPU\n",
    "\n",
    "# Originally:\n",
    "# my_model(params, inputs)\n",
    "# After:\n",
    "# jax_gptq(my_model)(params, inputs)\n",
    "quant_output = jax_gptq.use_quantized(my_model)(quantized_params, quant_data[0])\n",
    "\n",
    "print(f'Output of quantized network: {quant_output:.3e}')\n",
    "print(f'Original output: {original_output:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vXkTTctx7Vo"
   },
   "source": [
    "### Train with LoRA\n",
    "\n",
    "Now that we've compressed our model to 4-bits (and change) per parameter, we can add full precision LoRA parameters for finetuning.\n",
    "\n",
    "The one gotcha about combining the two is that Lorax doesn't know that QuantizedMatrix values are pytree leaves, so you need to give the Lorax functions an `is_leaf` predicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l95MirHdzNo9"
   },
   "source": [
    "**Initialization:** The `init_lora` function expects a pytree describing which parameters should get LoRA parameters, which should be fully trained, and which should be left frozen. `lorax.simple_spec` is a helper function for making these specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HKkhcjx9zJy6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_leaf(x):\n",
    "    \"\"\"\n",
    "    #목적: 이 함수는 주어진 객체가 QuantizedMatrix의 인스턴스인지 확인합니다.\n",
    "    # 이는 JAX의 tree_map 함수가 이 객체를 리프 노드로 인식하도록 하기 위함입니다.\n",
    "    \"\"\"\n",
    "    return isinstance(x, jax_gptq.QuantizedMatrix)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    목적: LoRA 사양을 정의합니다. 이는 양자화된 파라미터(quantized_params)에 대해 어떻게 LoRA 파라미터를 생성할지 결정합니다.\n",
    "\t•\tdecision_fn: 모든 파라미터에 대해 내적 랭크를 4로 지정합니다.\n",
    "\t•\ttune_vectors: 편향 벡터를 조정 가능하도록 하지 않고, 고정된 파라미터 트리에 배치합니다.\n",
    "\t•\tis_leaf: QuantizedMatrix 객체를 리프 노드로 인식합니다.\n",
    "\"\"\"\n",
    "\n",
    "lora_spec = lorax.simple_spec(\n",
    "    \n",
    "    params=quantized_params,\n",
    "    decision_fn=lambda pytree_path, arr: 4, # Just ignore the inputs and specify an inner rank of 4 for all params# 모든 파라미터에 대해 내적 랭크를 4로 지정\n",
    "    tune_vectors=False, # Tell Lorax to put all the biases in the frozen params tree instead of the tunable params tree\n",
    "     # 모든 편향을 조정 가능한 파라미터 트리 대신 고정된 파라미터 트리에 배치\n",
    "    is_leaf=is_leaf\n",
    ")\n",
    "\n",
    "# Lorax splits the parameters into two pytrees:\n",
    "# freeze_params: Anything which received the value lorax.LORA_FREEZE in the spec\n",
    "# train_params: Pairs of two narrow matrices for values which got positive integers as spec values, or the full parameter if the value lorax.LORA_FULL was in the spec\n",
    "\"\"\"\n",
    "•\t목적: LoRA 파라미터를 초기화합니다. 이 함수는 파라미터를 두 개의 pytree로 나눕니다:\n",
    "•\tfreeze_params: 고정된 파라미터. LoRA 사양에서 lorax.LORA_FREEZE 값을 받은 파라미터들.\n",
    "•\ttrain_params: 조정 가능한 파라미터. 사양에서 양의 정수 값을 받은 파라미터들.\n",
    "\"\"\"\n",
    "\n",
    "freeze_params, train_params = lorax.init_lora(param_tree = quantized_params, spec = lora_spec, rng =  jax.random.PRNGKey(1234), is_leaf=is_leaf)\n",
    "\n",
    "def merge_quantized_with_lora(q_params, lora_freeze):\n",
    "    return jax.tree_map(\n",
    "        lambda quant, from_lora: quant if isinstance(quant, jax_gptq.QuantizedMatrix) else from_lora,\n",
    "        q_params,\n",
    "        lora_freeze,\n",
    "        is_leaf=lambda x: isinstance(x, jax_gptq.QuantizedMatrix) # Tell tree_map to treat QuantizedMatrix as a single value instead of a non-leaf node\n",
    "    )\n",
    "# Now we put the actual quantized params back\n",
    "#freeze_params = merge_quantized_with_lora(quantized_params, freeze_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ebT9GXp16v4"
   },
   "source": [
    "The `lorax.lora` transform converts a function from expecting a single pytree in the specified argument to expecting a tuple of two pytrees. It composes with other JAX transforms such as `jax_gptq.use_quantized`, so we can use both at once with no modifications to our model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1XjjuQcq1oSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTQ + Lorax output: 8.691e+04\n",
      "GPTQ only: 8.691e+04\n"
     ]
    }
   ],
   "source": [
    "combined_params = (freeze_params, train_params)\n",
    "\n",
    "my_model_with_lora_and_quantized_weights = jax_gptq.use_quantized(lorax.lora(my_model))\n",
    "\n",
    "# The differences from the original `my_model` function are:\n",
    "# 1. The params argument now expects a tuple of (frozen_params, trainable_params)\n",
    "# 2. It knows how to compute with quantized weights\n",
    "quantized_plus_lorax_output = my_model_with_lora_and_quantized_weights(combined_params, quant_data[0])\n",
    "\n",
    "print(f'GPTQ + Lorax output: {quantized_plus_lorax_output:.3e}')\n",
    "print(f'GPTQ only: {quant_output:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIywP5qQ3KEH"
   },
   "source": [
    "The above values are identical since LoRA initializes one of each pair of matrices as zeros.\n",
    "\n",
    "Let's look at the size of each pytree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nqQwBPjh2ttl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.677e+05 frozen params\n",
      "1.024e+04 trainable params\n"
     ]
    }
   ],
   "source": [
    "count_params = partial(jax.tree_util.tree_reduce,\n",
    "  lambda acc, param: acc + (param.size if isinstance(param, jnp.ndarray) else 0),\n",
    "  initializer=0\n",
    ")\n",
    "\n",
    "print(f'{count_params(freeze_params):.3e} frozen params')\n",
    "print(f'{count_params(train_params):.3e} trainable params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CJ58F005g-c"
   },
   "source": [
    "Training with this function is no different from any other JAX function, just make sure to only differentiate your loss with respect to the trainable parameters only. (See the next section for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_lDOLnw5zoC"
   },
   "source": [
    "## GPT-Q-ing + LoRA-ing HuggingFace's Flax GPT-2\n",
    "I developed these transforms for use with my Haiku models, but since all JAX models are pure functions at the end of the day, it shouldn't matter what framework you use. Lorax supports matmuls and other matmul-like operations such as embedding lookups and 1-D convs.\n",
    "\n",
    "This is a minimal example of applying the combination to `gpt2-medium`, but it's basically model agnostic.\n",
    "\n",
    "First let's get the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "czS5kDWO6XTv"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VnfmpQ6f6Yal"
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2-medium'\n",
    "# 사전 학습된 모델에서 토크나이저를 불러옵니다. 이 토크나이저는 텍스트를 토큰으로 변환하는 데 사용됩니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#사전 학습된 모델을 Flax 형식으로 불러옵니다. _do_init=False는 모델을 초기화하지 않고 불러옵니다. 이는 이후 양자화를 위해 파라미터를 별도로 처리하기 위함입니다.\n",
    "model, params = FlaxAutoModelForCausalLM.from_pretrained(model_name, _do_init=False)\n",
    "#모델 파라미터를 CPU 장치에 배치합니다. 이는 GPU 메모리 사용을 줄이기 위해 초기 단계에서 파라미터를 CPU에 배치하는 것입니다.\n",
    "params = jax.device_put(params, cpu)\n",
    "\n",
    "#\t•\tparams['transformer']['wte']['embedding']: GPT-2 모델의 임베딩 테이블입니다. 입력 토큰을 임베딩 벡터로 변환하는 데 사용됩니다.\n",
    "#\t•\tnp.asarray(...): 임베딩 테이블을 numpy 배열로 변환하여 저장합니다. 이는 양자화 과정에서 임베딩 테이블이 손상되는 것을 방지하기 위함입니다. 양자화 후에 임베딩 테이블을 다시 사용할 수 있도록 별도로 저장해 둡니다.\n",
    "# Because the embedding table is reused as the output linear layer, it'll get quantized at the end of the process, but that will seriously screw up the embedding lookup step, so we'll just save it for later here\n",
    "orig_embedding_table = np.asarray(params['transformer']['wte']['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evCyWa787m_N"
   },
   "source": [
    "The GPT-Q paper used real text data for quantization, but for this demo I'll just generate some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ao_vTWAf7Tw-"
   },
   "outputs": [],
   "source": [
    "QUANT_BATCH_SIZE = 4 #\t•\tQUANT_BATCH_SIZE: 양자화를 위해 사용할 배치 크기입니다. 여기서는 4로 설정되어 있습니다.\n",
    "#양자화 예제의 길이입니다. 각 예제는 64개의 토큰으로 구성됩니다. 이 값을 더 크게 설정할 수 있지만, Colab에서 메모리 충돌을 방지하기 위해 작은 값으로 설정되었습니다\n",
    "QUANT_EXAMPLE_LENGTH = 64 # I'd recommend making this bigger, but needs to be small to not crash colab\n",
    "\n",
    "quantization_data = []\n",
    "key = jax.random.PRNGKey(0) #JAX의 랜덤 키를 초기화합니다. 랜덤 키는 재현 가능한 무작위 값을 생성하는 데 사용됩니다.\n",
    "for _ in range(32):\n",
    "  #jax.random.randint(key, (QUANT_BATCH_SIZE, QUANT_EXAMPLE_LENGTH), 0, 50256): 무작위 정수로 구성된 텐서를 생성합니다. 각 배치는 QUANT_BATCH_SIZE x QUANT_EXAMPLE_LENGTH 크기의 텐서입니다. 각 값은 0에서 50255 사이의 정수입니다 (50256은 GPT-2의 단어 집합 크기입니다).\n",
    "  batch = jax.random.randint(key, (QUANT_BATCH_SIZE, QUANT_EXAMPLE_LENGTH), 0, 50256)\n",
    "  quantization_data.append(batch) #quantization_data.append(batch): 생성된 배치를 양자화 데이터 리스트에 추가합니다.\n",
    "  key, = jax.random.split(key, 1) #랜덤 키를 업데이트하여 다음 배치를 생성할 때 사용할 새로운 키를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(type(quantization_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x_pT_fT8Co8"
   },
   "source": [
    "HuggingFace's models don't have quite the right call signature, so we'll make a wrapper which takes (params, inputs) as an argument:\n",
    "주어진 코드는 HuggingFace 모델의 호출 시그니처를 변경하여 JAX 기반의 양자화 함수에 맞게 래퍼를 작성하고, 양자화된 모델 파라미터를 원래의 임베딩 테이블로 대체하는 과정을 포함합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "yddz4OUN8Bvt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 0it [00:00, ?it/s]/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:352: UserWarning: Operation gather not supported for quantization\n",
      "  warnings.warn(f'Operation {eqn.primitive.name} not supported for quantization')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 3.28e+04 bytes\n",
      "Current param env size: 2.23e+08 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 2it [00:38, 19.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 3it [01:13, 25.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.72e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 4it [01:51, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 5it [03:50, 61.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.72e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 6it [03:58, 43.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 7it [04:06, 32.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.72e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 8it [04:15, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 9it [04:36, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.72e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 10it [04:44, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 11it [04:52, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.72e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 12it [05:00, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 13it [05:18, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.72e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 14it [05:24, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 15it [05:31, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.73e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 16it [05:39,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 17it [05:57, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.73e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 18it [06:04, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 19it [06:10,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.73e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 20it [06:18,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 21it [06:38, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.73e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 22it [06:47, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 23it [06:54, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.73e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 24it [07:02,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 25it [07:23, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.73e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 26it [07:31, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 27it [07:38, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.74e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 28it [07:46,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 29it [08:05, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.74e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 30it [08:14, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 31it [08:22, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.74e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 32it [08:31,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 33it [08:51, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.74e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 34it [08:59, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 35it [09:07, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.74e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 36it [09:15,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 37it [09:37, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.74e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 38it [09:45, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 39it [09:53, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.75e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 40it [10:02, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 41it [10:22, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.75e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 42it [10:30, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 43it [10:37, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.75e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 44it [10:45,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 45it [11:02, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.75e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 46it [11:09, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 47it [11:14,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.75e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 48it [11:20,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 49it [11:33,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.75e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 50it [11:39,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 51it [11:46,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.76e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 52it [11:52,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 53it [12:11, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.76e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 54it [12:20, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 55it [12:27,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.76e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 56it [12:35,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 57it [12:55, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.76e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 58it [13:04, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 59it [13:12, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.76e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 60it [13:21,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 61it [13:42, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.76e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 62it [13:51, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 63it [13:59, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.77e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 64it [14:08, 10.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 65it [14:29, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.77e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 66it [14:37, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 67it [14:45, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.77e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 68it [14:52,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 69it [15:13, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.77e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 70it [15:22, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 71it [15:30, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.77e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 72it [15:39, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 73it [15:56, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.77e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 74it [16:04, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 75it [16:11,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.78e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 76it [16:18,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 77it [16:38, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.78e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 78it [16:47, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 79it [16:55, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.78e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 80it [17:03,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 81it [17:22, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.78e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 82it [17:30, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 83it [17:38, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.78e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 84it [17:45,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.68e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 85it [18:04, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.78e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 86it [18:13, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.69e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 87it [18:20,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.79e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 88it [18:28,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.69e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 89it [18:48, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.79e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 90it [18:56, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.69e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 91it [19:04, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.79e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 92it [19:12,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.69e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 93it [19:31, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.79e+07 bytes\n",
      "Current param env size: 1.26e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 94it [19:38, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.69e+08 bytes\n",
      "Current param env size: 4.21e+06 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 95it [19:46,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.79e+07 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 96it [19:54,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.69e+08 bytes\n",
      "Current param env size: 1.68e+07 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 97it [20:13, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 6.79e+07 bytes\n",
      "Current param env size: 2.06e+08 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0724 18:44:17.280217 2540469 pjrt_stream_executor_client.cc:2985] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to allocate request for 49.08MiB (51463168B) on device ordinal 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RESOURCE_EXHAUSTED: Failed to allocate request for 49.08MiB (51463168B) on device ordinal 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model(batch, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 작성한 래퍼 함수와 함께 GPT-Q 양자화 함수를 호출하여 양자화된 파라미터를 생성합니다.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\t•\tjax_gptq.quantize 함수 호출: 작성한 래퍼 함수(apply_model), 모델 파라미터(params), 양자화 데이터(quantization_data)를 사용하여 GPT-Q 알고리즘을 통해 모델 파라미터를 양자화합니다.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# •\tquantized_params: 양자화된 파라미터가 저장됩니다.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m quantized_params \u001b[38;5;241m=\u001b[39m jax_gptq\u001b[38;5;241m.\u001b[39mquantize(apply_model, params, quantization_data)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:74\u001b[0m, in \u001b[0;36mquantize\u001b[0;34m(fn, params, inputs, block_size, actorder, damping, use_quantized_activations, use_fp64, use_params_fp32)\u001b[0m\n\u001b[1;32m     72\u001b[0m argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(param_args)))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# `_eval_and_quantize` 함수를 호출하여 양자화된 파라미터를 생성\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m result \u001b[38;5;241m=\u001b[39m _eval_and_quantize(\n\u001b[1;32m     75\u001b[0m     closed_jaxpr\u001b[38;5;241m.\u001b[39mjaxpr,\n\u001b[1;32m     76\u001b[0m     closed_jaxpr\u001b[38;5;241m.\u001b[39mliterals,\n\u001b[1;32m     77\u001b[0m     argnums,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39mparam_args,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39minput_args,\n\u001b[1;32m     80\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m     81\u001b[0m     actorder\u001b[38;5;241m=\u001b[39mactorder,\n\u001b[1;32m     82\u001b[0m     damping\u001b[38;5;241m=\u001b[39mdamping,\n\u001b[1;32m     83\u001b[0m     use_quantized_activations\u001b[38;5;241m=\u001b[39muse_quantized_activations,\n\u001b[1;32m     84\u001b[0m     use_fp64\u001b[38;5;241m=\u001b[39muse_fp64,\n\u001b[1;32m     85\u001b[0m     use_params_fp32\u001b[38;5;241m=\u001b[39muse_params_fp32\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 생성된 양자화된 파라미터를 원래 파라미터 리스트에 다시 할당\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, quantized_param \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:266\u001b[0m, in \u001b[0;36m_eval_and_quantize\u001b[0;34m(jaxpr, consts, argnums, block_size, actorder, damping, use_quantized_activations, use_fp64, use_params_fp32, *args)\u001b[0m\n\u001b[1;32m    259\u001b[0m gpu_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    260\u001b[0m     matmul_w_arg\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argname \u001b[38;5;241m==\u001b[39m quantize_argname \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     env[argname]\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m argname \u001b[38;5;129;01min\u001b[39;00m matmul_eqn\u001b[38;5;241m.\u001b[39minvars\n\u001b[1;32m    264\u001b[0m ]\n\u001b[1;32m    265\u001b[0m gpu_args \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_put(gpu_args, gpu)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m do_eval(\u001b[38;5;241m*\u001b[39mgpu_args)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree_size_bytes(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e8\u001b[39m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# This should offload stuff like the final logits to the CPU\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     cpu_results \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_put(results, cpu)\n",
      "\u001b[0;31mValueError\u001b[0m: RESOURCE_EXHAUSTED: Failed to allocate request for 49.08MiB (51463168B) on device ordinal 0"
     ]
    }
   ],
   "source": [
    "# HuggingFace의 모델 호출 시그니처가 JAX의 요구 사항과 맞지 않기 때문에,\n",
    "# (params, inputs) 형식의 인수를 받도록 래퍼를 작성합니다.\n",
    "def apply_model(params, batch):\n",
    "  return model(batch, params=params)\n",
    "# 작성한 래퍼 함수와 함께 GPT-Q 양자화 함수를 호출하여 양자화된 파라미터를 생성합니다.\n",
    "#\t•\tjax_gptq.quantize 함수 호출: 작성한 래퍼 함수(apply_model), 모델 파라미터(params), 양자화 데이터(quantization_data)를 사용하여 GPT-Q 알고리즘을 통해 모델 파라미터를 양자화합니다.\n",
    "# •\tquantized_params: 양자화된 파라미터가 저장됩니다.\n",
    "quantized_params = jax_gptq.quantize(apply_model, params, quantization_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ehblO3I98akJ"
   },
   "outputs": [],
   "source": [
    "# Replace the quantized embedding table with the original one\n",
    "#양자화된 임베딩 테이블을 원래의 임베딩 테이블로 대체합니다.\n",
    "#\t•\t목적: 임베딩 테이블은 입력 토큰을 벡터로 변환하는 중요한 역할을 합니다. 양자화 과정에서 임베딩 테이블이 손상될 수 있으므로, 원래의 임베딩 테이블로 대체하여 정확성을 유지합니다.\n",
    "#   •\tquantized_params['transformer']['wte']['embedding']: 양자화된 파라미터에서 임베딩 테이블을 찾아서 원래의 임베딩 테이블(orig_embedding_table)로 대체합니다.\n",
    "quantized_params['transformer']['wte']['embedding'] = jnp.asarray(orig_embedding_table)\n",
    "quantized_params = jax.device_put(quantized_params, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYiCG5fE9yKT"
   },
   "source": [
    "### Finetuning GPT-2 with Lorax\n",
    "\n",
    "Same as [above](https://colab.research.google.com/drive/18rkULbWqk7mNZDx7Scx-JS3p_s45mgok#scrollTo=HKkhcjx9zJy6&line=3&uniqifier=1), we get the original param structure to tell Lorax how to initialize the LoRA params, then merge the quantized params back in after.\n",
    "\n",
    "이 코드는 LoRA(Low-Rank Adaptation) 기법을 사용하여 양자화된 GPT-2 모델을 미세 조정하는 과정입니다. LoRA는 모델의 일부 파라미터만을 저순위 행렬로 미세 조정함으로써, 메모리 사용량과 계산량을 줄이는 기법입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FKS_dfll93sO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1318360/3777058010.py:38: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(\n"
     ]
    }
   ],
   "source": [
    "# Get pre-quantization param tree (some nodes will just be abstract values)\n",
    "orig_params_or_shapes = jax_gptq.utils.quantized_params_to_shaped_arrays(quantized_params)\n",
    "\n",
    "# Tell Lorax which leaves should be frozen/fully trained/LoRA trained\n",
    "# LoRA 사양을 정의하여 어느 파라미터가 고정되고, 완전 훈련되며, LoRA로 훈련될지 지정합니다.\n",
    "#\t•\tsimple_spec 함수:\n",
    "#   •\torig_params_or_shapes: 원래 파라미터 구조.\n",
    "#   •\tlambda path, arr: 경로에 c_attn 또는 mlp 패턴이 포함된 경우 내적 랭크를 16으로 설정하고, 그렇지 않으면 파라미터를 고정합니다.\n",
    "#   •\ttune_vectors=True: 벡터를 조정하도록 설정합니다.\n",
    "\n",
    "spec = lorax.simple_spec(\n",
    "    orig_params_or_shapes,\n",
    "    lambda path, arr: 16 if any(pattern in path for pattern in ['c_attn', 'mlp']) else lorax.LORA_FREEZE,\n",
    "    tune_vectors=True\n",
    ")\n",
    "\n",
    "# Initialize parameters\n",
    "\"\"\"\n",
    "\t•\t목적: LoRA 파라미터를 초기화합니다.\n",
    "\t•\tjax.random.split 함수: 랜덤 키를 분할하여 초기화 키를 생성합니다.\n",
    "\t•\tinit_lora 함수:\n",
    "\t•\torig_params_or_shapes: 원래 파라미터 구조.\n",
    "\t•\tspec: 정의된 LoRA 사양.\n",
    "\t•\tinit_key: 파라미터 초기화에 사용할 랜덤 키.\n",
    "\n",
    "\"\"\"\n",
    "key, init_key = jax.random.split(key)\n",
    "freeze_params, train_params = lorax.init_lora(\n",
    "    orig_params_or_shapes,\n",
    "    spec,\n",
    "    init_key\n",
    ")\n",
    "# 양자화된 파라미터를 고정된 파라미터 트리에 다시 병합합니다.\n",
    "# Put the quantized params back into the frozen param tree\n",
    "freeze_params = merge_quantized_with_lora(quantized_params, freeze_params)\n",
    "combined_params = freeze_params, train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8bJwqN2Bfqh"
   },
   "source": [
    "Now we can just transform the `apply_model` function and it will use both LoRA and 4-bit quantized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "glARn7Z0BX4g"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\t•\t목적: apply_model 함수를 LoRA와 4-bit 양자화된 파라미터를 모두 사용하도록 변환합니다.\n",
    "\t•\tlorax.lora(apply_model): LoRA 파라미터를 사용하는 함수로 변환합니다.\n",
    "\t•\tjax_gptq.use_quantized: 4-bit 양자화된 파라미터를 사용하는 함수로 변환합니다.\n",
    "\"\"\"\n",
    "\n",
    "quantized_plus_lora_fn = jax_gptq.use_quantized(lorax.lora(apply_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1G-d0yDBn8y"
   },
   "source": [
    "### Training\n",
    "Training isn't actually any different from normal training, since you can just think of `freeze_params` as being a constant argument, but here's a demo for completness.\n",
    "\n",
    "First I'll define a toy corpus which demonstrates Alan's love of cats and Grace's dislike of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I3fdjSioBvDO"
   },
   "outputs": [],
   "source": [
    "# 고양이와 개를 좋아하는 인물에 대한 간단한 텍스트 데이터셋을 만듭니다.\n",
    "CATS = ['lions', 'tigers', 'cheetahs', 'cats', 'ocelots', 'kittens']\n",
    "DOGS = ['wolves', 'dogs', 'coyotes', 'huskies', 'poodles', 'puppies']\n",
    "\n",
    "CAT_LOVER = 'Alan'\n",
    "DOG_LOVER = 'Grace'\n",
    "\n",
    "dataset = []\n",
    "for name, polarity in [(CAT_LOVER, True), (DOG_LOVER, False)]:\n",
    "  liked, disliked = (CATS, DOGS) if polarity else (DOGS, CATS)\n",
    "  for kind in liked:\n",
    "    dataset.append(f'{name}: {kind}? I love them!')\n",
    "    dataset.append(f'{name}: Hey look at those {kind}, that\\'s pretty cool')\n",
    "\n",
    "  for kind in disliked:\n",
    "    dataset.append(f'{name}: {kind}? I hate them!')\n",
    "    dataset.append(f'{name}: Oh no, some {kind}! How scary!')\n",
    "# 텍스트 데이터를 토큰화하고, 최대 길이에 맞춰 패딩합니다.\n",
    "tokenized_data = [jnp.asarray(tokenizer.encode(ex)) for ex in dataset]\n",
    "max_len = max(ex.shape[0] for ex in tokenized_data)\n",
    "# Pad the data to speed up jitting. Not worrying about masking due to laziness.\n",
    "# 패딩을 통해 데이터를 동일한 길이로 맞춥니다. 마스킹은 생략합니다.\n",
    "tokenized_data = [jnp.pad(ex, (0, max_len - ex.shape[0])) for ex in tokenized_data]\n",
    "\"\"\"\n",
    "\t•\t목적: 변환된 모델을 JIT 컴파일하여 성능을 최적화합니다.\n",
    "\t•\tjax.jit: JAX 함수의 JIT 컴파일러를 사용하여 모델을 컴파일합니다.\n",
    "\"\"\"\n",
    "jitted_model = jax.jit(quantized_plus_lora_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NZFLWJgxYqfh"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\t•\t목적: 주어진 프리픽스를 사용하여 모델의 예측을 출력하는 함수를 정의합니다.\n",
    "\t•\ttokenizer.encode: 프리픽스를 토큰화합니다.\n",
    "\t•\tjitted_model(params, tokens[None]): JIT 컴파일된 모델을 사용하여 로짓(logits)을 계산합니다.\n",
    "\t•\tjax.nn.log_softmax: 로짓에 소프트맥스 함수를 적용하여 확률 분포를 계산합니다.\n",
    "\t•\tjax.lax.top_k: 상위 5개의 예측 단어와 그 확률을 찾습니다.\n",
    "\t•\t예측 결과 출력: 프리픽스에 대한 예측 결과를 출력합니다.\n",
    "\n",
    "\"\"\"\n",
    "def make_prediction(params, prefix):\n",
    "  tokens = jnp.asarray(tokenizer.encode(prefix))\n",
    "  logits = jitted_model(params, tokens[None]).logits\n",
    "  \n",
    "  logprobs = jnp.exp(jax.nn.log_softmax(logits[0, -1]))\n",
    "  pred_probs, pred_words = jax.lax.top_k(logprobs, 5)\n",
    "\n",
    "  print(f'Predictions for: \"{prefix}\"')\n",
    "  for i, (word_id, prob) in enumerate(zip(pred_words, pred_probs), 1):\n",
    "    print(f'{i}. {tokenizer.decode([word_id])} - {prob:.2%}')\n",
    "  print()\n",
    "\n",
    "test_examples = [\n",
    "    f'{CAT_LOVER}: jaguars? I',\n",
    "    f'{DOG_LOVER}: jaguars? I'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT7hOBnYS-AC"
   },
   "source": [
    "Let's look at the next word predictions of the unmodified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eew7ihGJTD85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 11:32:30.118895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 11:32:30.192666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 11:32:30.193099: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-23 11:32:31.438438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for: \"Alan: jaguars? I\"\n",
      "1. 'm - 11.25%\n",
      "2.  mean - 8.90%\n",
      "3. 've - 7.17%\n",
      "4.  don - 6.74%\n",
      "5.  thought - 4.55%\n",
      "\n",
      "Predictions for: \"Grace: jaguars? I\"\n",
      "1. 'm - 10.07%\n",
      "2.  don - 7.90%\n",
      "3.  mean - 6.11%\n",
      "4. 've - 6.09%\n",
      "5.  thought - 4.36%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ex in test_examples:\n",
    "  make_prediction(combined_params, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrSL1MgSDXfO"
   },
   "source": [
    "Next we set up a standard training loop. The only difference is that we keep the train/freeze params separate for the optimizer. There's no differences needed for the quantization.\n",
    "\n",
    "I'll just train with a batch size of 1 here since I don't want to bother with masking, but the transformed model function is fully compatible with vmap etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "52QdkmIxDHk-"
   },
   "outputs": [],
   "source": [
    "def loss_fn(train_params, freeze_params, seq):\n",
    "  inputs = seq[:-1]\n",
    "  targets = seq[1:]\n",
    "\n",
    "  combined_params = (freeze_params, train_params)\n",
    "  logits = quantized_plus_lora_fn(combined_params, inputs[None]).logits[0]\n",
    "  logprobs = jax.nn.log_softmax(logits)\n",
    "  losses = -jnp.take_along_axis(logprobs, targets[:, None], axis=-1)\n",
    "  return jnp.mean(losses)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=1e-4, weight_decay=1e-4)\n",
    "opt_state = optimizer.init(combined_params[1])\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(combined_params, opt_state, example):\n",
    "  freeze_params, train_params = combined_params\n",
    "\n",
    "  # The main thing is that we have to split up the params here so that JAX knows what to differentiate with respect to\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(train_params, freeze_params, example)\n",
    "\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params=train_params)\n",
    "  new_train_params = optax.apply_updates(train_params, updates)\n",
    "  return (freeze_params, new_train_params), opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cj2d1xIqFJw3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Loss: 2.581e-01: 100%|██████████| 50/50 [34:29<00:00, 41.38s/it] \n"
     ]
    }
   ],
   "source": [
    "bar = trange(50)\n",
    "for epoch in bar:\n",
    "  key, = jax.random.split(key, 1)\n",
    "  permutation = jax.random.permutation(key, jnp.arange(len(dataset)))\n",
    "  total_loss = 0\n",
    "  for index in permutation:\n",
    "    example = tokenized_data[index]\n",
    "    combined_params, opt_state, loss = update_fn(combined_params, opt_state, example)\n",
    "    total_loss += loss\n",
    "  bar.set_description(f'Epoch {epoch} - Loss: {total_loss / len(tokenized_data):.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMFZwE8qeSUl"
   },
   "source": [
    "The trained LoRA parameters give us a model which predicts that Alan will love jaguars, and Grace will hate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GIgThnapFQS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for: \"Alan: jaguars? I\"\n",
      "1.  love - 83.57%\n",
      "2.  hate - 16.42%\n",
      "3.  LOVE - 0.00%\n",
      "4.  like - 0.00%\n",
      "5.  want - 0.00%\n",
      "\n",
      "\n",
      "Predictions for: \"Grace: jaguars? I\"\n",
      "1.  hate - 62.58%\n",
      "2.  love - 37.39%\n",
      "3.  LOVE - 0.01%\n",
      "4. 'll - 0.01%\n",
      "5.  Hate - 0.01%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in test_examples:\n",
    "  make_prediction(combined_params, example)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92W8jCjQeZ9J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Y6JeyF45yd_"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
