{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiSi90gspEQP"
   },
   "source": [
    "# Easy GPT-Q + LoRA in JAX ([github](https://github.com/davisyoshida/easy-lora-and-gptq))\n",
    "\n",
    "[Davis Yoshida](https://github.com/davisyoshida/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfxALa1so2JD"
   },
   "source": [
    "This notebook shows how to combine  two JAX tools/transforms I wrote: [Lorax](https://github.com/davisyoshida/lorax) and [JAX-GPTQ](https://github.com/davisyoshida/jax-gptq). I've been using the combination to run LLaMA finetunes on a single GPU.\n",
    "\n",
    "They're both applicable to basically any JAX function, which conveniently includes many HuggingFace models!\n",
    "\n",
    "The procedure is as follows:\n",
    "\n",
    "1. Quantize the weights of the model we want to use\n",
    "2. Use Lorax to transform the original model function `F(params, inputs)` to one that takes a tuple of the original params and the low rank LoRA params: `F_lora(param_tuple, inputs)`\n",
    "3. Wrap `F_lora` in `use_quantized` transform so that it knows how to handle arguments which are int8 matrices with two parameters per byte.\n",
    "4. Train the model, updating only the low rank params and leaving the larger 4-bit model weights frozen.\n",
    "\n",
    "I'd love feedback on one or both of these tools so please let me know on their Githubs if you have any suggestions. JAX-GPTQ in particular is still in a really early state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y6JeyF45yd_"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "75-T_R0Ms9qD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "# 아래 코드는 원하는 GPU 번호만 쓰도록 설정하는 코드\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "import lorax\n",
    "import jax_gptq\n",
    "\n",
    "gpu = jax.devices('gpu')[0]\n",
    "cpu = jax.devices('cpu')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQuDSjz7svdL"
   },
   "source": [
    "## Toy Example\n",
    "\n",
    "### Model/Data setup\n",
    "\n",
    "First we'll define an MLP and make some parameters for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Djyo_reAs26R"
   },
   "outputs": [],
   "source": [
    "N_LAYER = 5\n",
    "batch_size = 16\n",
    "DIM = 256\n",
    "\n",
    "def my_model(params, x):\n",
    "  for layer in params:\n",
    "    x = jax.nn.relu(x @ layer['w'] + layer['b'])\n",
    "\n",
    "  return jnp.mean(x)\n",
    "\n",
    "w_key, b_key, data_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "\n",
    "w_keys = jax.random.split(w_key, N_LAYER)\n",
    "b_keys = jax.random.split(b_key, N_LAYER)\n",
    "\n",
    "# Make some params\n",
    "params = [\n",
    "    {\n",
    "        'w': jax.random.normal(k1, (DIM, DIM)),\n",
    "        'b': jax.random.normal(k2, (DIM,))\n",
    "    }\n",
    "    for k1, k2 in zip(w_keys, b_keys)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlCLAmjBvhnA"
   },
   "source": [
    "GPT-Q needs input data for quantization. For an actual model we'd use real data but here we'll just make some random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6govTMOZvgSC"
   },
   "outputs": [],
   "source": [
    "quant_data = [jax.random.normal(key, (batch_size, DIM)) for key in jax.random.split(data_key, 16)]\n",
    "\n",
    "# We'll save an output for later comparison since the quantization process will delete the original params\n",
    "original_output = my_model(params, quant_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjdb3h46vtsi"
   },
   "source": [
    "### Run GPT-Q to get the quantized weights\n",
    "That's all for the setup, we can now just run GPT-Q (without any changes to the original model code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "L1Mw9ZLpvrLa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 0.00e+00 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 2it [00:15,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 3it [00:16,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 4it [00:17,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 5it [00:18,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 2.62e+05 bytes\n",
      "Current param env size: 1.02e+03 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 6it [00:19,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Note that this may free the buffers associated with some or all of the parameters and the data to save VRAM\n",
    "# I'd also recommend you put the params on the CPU, since `quantize()` will move the params to th GPU when necessary\n",
    "quantized_params = jax_gptq.quantize(my_model, params, quant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NhVv8egwDQu"
   },
   "source": [
    "The matrices have been quantized but the biases have been left alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bWwXzTJyubbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W type: <class 'jax_gptq.gptq.QuantizedMatrix'>\n",
      "B type: <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(f'W type: {type(quantized_params[0][\"w\"])}')\n",
    "print(f'B type: {type(quantized_params[0][\"b\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwYLTr6WwapB"
   },
   "source": [
    "**Note**: The quantization procedure depends on the parameter being used in a matrix multiplication. Currently JAX-GPTQ supports general dot operations (including ones using tensors with any number of dimensions larger than 1), and convolutions with kernels of spatial size 1.\n",
    "\n",
    "### Applying the quantized weights\n",
    "We can now run the quantized model without any code changes. All that's necessary is using `jax_gptq.use_quantized` to transform the function so it knows how to handle `QuantizedMatrix` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I6aLdXqawQFs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of quantized network: 8.691e+04\n",
      "Original output: 8.575e+04\n"
     ]
    }
   ],
   "source": [
    "quantized_params = jax.device_put(quantized_params, gpu) # Move the params to the GPU\n",
    "\n",
    "# Originally:\n",
    "# my_model(params, inputs)\n",
    "# After:\n",
    "# jax_gptq(my_model)(params, inputs)\n",
    "quant_output = jax_gptq.use_quantized(my_model)(quantized_params, quant_data[0])\n",
    "\n",
    "print(f'Output of quantized network: {quant_output:.3e}')\n",
    "print(f'Original output: {original_output:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vXkTTctx7Vo"
   },
   "source": [
    "### Train with LoRA\n",
    "\n",
    "Now that we've compressed our model to 4-bits (and change) per parameter, we can add full precision LoRA parameters for finetuning.\n",
    "\n",
    "The one gotcha about combining the two is that Lorax doesn't know that QuantizedMatrix values are pytree leaves, so you need to give the Lorax functions an `is_leaf` predicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l95MirHdzNo9"
   },
   "source": [
    "**Initialization:** The `init_lora` function expects a pytree describing which parameters should get LoRA parameters, which should be fully trained, and which should be left frozen. `lorax.simple_spec` is a helper function for making these specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HKkhcjx9zJy6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_leaf(x):\n",
    "    \"\"\"\n",
    "    #목적: 이 함수는 주어진 객체가 QuantizedMatrix의 인스턴스인지 확인합니다.\n",
    "    # 이는 JAX의 tree_map 함수가 이 객체를 리프 노드로 인식하도록 하기 위함입니다.\n",
    "    \"\"\"\n",
    "    return isinstance(x, jax_gptq.QuantizedMatrix)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    목적: LoRA 사양을 정의합니다. 이는 양자화된 파라미터(quantized_params)에 대해 어떻게 LoRA 파라미터를 생성할지 결정합니다.\n",
    "\t•\tdecision_fn: 모든 파라미터에 대해 내적 랭크를 4로 지정합니다.\n",
    "\t•\ttune_vectors: 편향 벡터를 조정 가능하도록 하지 않고, 고정된 파라미터 트리에 배치합니다.\n",
    "\t•\tis_leaf: QuantizedMatrix 객체를 리프 노드로 인식합니다.\n",
    "\"\"\"\n",
    "\n",
    "lora_spec = lorax.simple_spec(\n",
    "    \n",
    "    params=quantized_params,\n",
    "    decision_fn=lambda pytree_path, arr: 4, # Just ignore the inputs and specify an inner rank of 4 for all params# 모든 파라미터에 대해 내적 랭크를 4로 지정\n",
    "    tune_vectors=False, # Tell Lorax to put all the biases in the frozen params tree instead of the tunable params tree\n",
    "     # 모든 편향을 조정 가능한 파라미터 트리 대신 고정된 파라미터 트리에 배치\n",
    "    is_leaf=is_leaf\n",
    ")\n",
    "\n",
    "# Lorax splits the parameters into two pytrees:\n",
    "# freeze_params: Anything which received the value lorax.LORA_FREEZE in the spec\n",
    "# train_params: Pairs of two narrow matrices for values which got positive integers as spec values, or the full parameter if the value lorax.LORA_FULL was in the spec\n",
    "\"\"\"\n",
    "•\t목적: LoRA 파라미터를 초기화합니다. 이 함수는 파라미터를 두 개의 pytree로 나눕니다:\n",
    "•\tfreeze_params: 고정된 파라미터. LoRA 사양에서 lorax.LORA_FREEZE 값을 받은 파라미터들.\n",
    "•\ttrain_params: 조정 가능한 파라미터. 사양에서 양의 정수 값을 받은 파라미터들.\n",
    "\"\"\"\n",
    "\n",
    "freeze_params, train_params = lorax.init_lora(param_tree = quantized_params, spec = lora_spec, rng =  jax.random.PRNGKey(1234), is_leaf=is_leaf)\n",
    "\n",
    "def merge_quantized_with_lora(q_params, lora_freeze):\n",
    "    return jax.tree_map(\n",
    "        lambda quant, from_lora: quant if isinstance(quant, jax_gptq.QuantizedMatrix) else from_lora,\n",
    "        q_params,\n",
    "        lora_freeze,\n",
    "        is_leaf=lambda x: isinstance(x, jax_gptq.QuantizedMatrix) # Tell tree_map to treat QuantizedMatrix as a single value instead of a non-leaf node\n",
    "    )\n",
    "# Now we put the actual quantized params back\n",
    "#freeze_params = merge_quantized_with_lora(quantized_params, freeze_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ebT9GXp16v4"
   },
   "source": [
    "The `lorax.lora` transform converts a function from expecting a single pytree in the specified argument to expecting a tuple of two pytrees. It composes with other JAX transforms such as `jax_gptq.use_quantized`, so we can use both at once with no modifications to our model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1XjjuQcq1oSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTQ + Lorax output: 8.691e+04\n",
      "GPTQ only: 8.691e+04\n"
     ]
    }
   ],
   "source": [
    "combined_params = (freeze_params, train_params)\n",
    "\n",
    "my_model_with_lora_and_quantized_weights = jax_gptq.use_quantized(lorax.lora(my_model))\n",
    "\n",
    "# The differences from the original `my_model` function are:\n",
    "# 1. The params argument now expects a tuple of (frozen_params, trainable_params)\n",
    "# 2. It knows how to compute with quantized weights\n",
    "quantized_plus_lorax_output = my_model_with_lora_and_quantized_weights(combined_params, quant_data[0])\n",
    "\n",
    "print(f'GPTQ + Lorax output: {quantized_plus_lorax_output:.3e}')\n",
    "print(f'GPTQ only: {quant_output:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIywP5qQ3KEH"
   },
   "source": [
    "The above values are identical since LoRA initializes one of each pair of matrices as zeros.\n",
    "\n",
    "Let's look at the size of each pytree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nqQwBPjh2ttl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.677e+05 frozen params\n",
      "1.024e+04 trainable params\n"
     ]
    }
   ],
   "source": [
    "count_params = partial(jax.tree_util.tree_reduce,\n",
    "  lambda acc, param: acc + (param.size if isinstance(param, jnp.ndarray) else 0),\n",
    "  initializer=0\n",
    ")\n",
    "\n",
    "print(f'{count_params(freeze_params):.3e} frozen params')\n",
    "print(f'{count_params(train_params):.3e} trainable params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CJ58F005g-c"
   },
   "source": [
    "Training with this function is no different from any other JAX function, just make sure to only differentiate your loss with respect to the trainable parameters only. (See the next section for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_lDOLnw5zoC"
   },
   "source": [
    "## GPT-Q-ing + LoRA-ing HuggingFace's Flax GPT-2\n",
    "I developed these transforms for use with my Haiku models, but since all JAX models are pure functions at the end of the day, it shouldn't matter what framework you use. Lorax supports matmuls and other matmul-like operations such as embedding lookups and 1-D convs.\n",
    "\n",
    "This is a minimal example of applying the combination to `gpt2-medium`, but it's basically model agnostic.\n",
    "\n",
    "First let's get the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "czS5kDWO6XTv"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VnfmpQ6f6Yal"
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2-medium'\n",
    "# 사전 학습된 모델에서 토크나이저를 불러옵니다. 이 토크나이저는 텍스트를 토큰으로 변환하는 데 사용됩니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#사전 학습된 모델을 Flax 형식으로 불러옵니다. _do_init=False는 모델을 초기화하지 않고 불러옵니다. 이는 이후 양자화를 위해 파라미터를 별도로 처리하기 위함입니다.\n",
    "model, params = FlaxAutoModelForCausalLM.from_pretrained(model_name, _do_init=False)\n",
    "#모델 파라미터를 CPU 장치에 배치합니다. 이는 GPU 메모리 사용을 줄이기 위해 초기 단계에서 파라미터를 CPU에 배치하는 것입니다.\n",
    "params = jax.device_put(params, cpu)\n",
    "\n",
    "#\t•\tparams['transformer']['wte']['embedding']: GPT-2 모델의 임베딩 테이블입니다. 입력 토큰을 임베딩 벡터로 변환하는 데 사용됩니다.\n",
    "#\t•\tnp.asarray(...): 임베딩 테이블을 numpy 배열로 변환하여 저장합니다. 이는 양자화 과정에서 임베딩 테이블이 손상되는 것을 방지하기 위함입니다. 양자화 후에 임베딩 테이블을 다시 사용할 수 있도록 별도로 저장해 둡니다.\n",
    "# Because the embedding table is reused as the output linear layer, it'll get quantized at the end of the process, but that will seriously screw up the embedding lookup step, so we'll just save it for later here\n",
    "orig_embedding_table = np.asarray(params['transformer']['wte']['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evCyWa787m_N"
   },
   "source": [
    "The GPT-Q paper used real text data for quantization, but for this demo I'll just generate some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ao_vTWAf7Tw-"
   },
   "outputs": [],
   "source": [
    "QUANT_BATCH_SIZE = 4 #\t•\tQUANT_BATCH_SIZE: 양자화를 위해 사용할 배치 크기입니다. 여기서는 4로 설정되어 있습니다.\n",
    "#양자화 예제의 길이입니다. 각 예제는 64개의 토큰으로 구성됩니다. 이 값을 더 크게 설정할 수 있지만, Colab에서 메모리 충돌을 방지하기 위해 작은 값으로 설정되었습니다\n",
    "QUANT_EXAMPLE_LENGTH = 64 # I'd recommend making this bigger, but needs to be small to not crash colab\n",
    "\n",
    "quantization_data = []\n",
    "key = jax.random.PRNGKey(0) #JAX의 랜덤 키를 초기화합니다. 랜덤 키는 재현 가능한 무작위 값을 생성하는 데 사용됩니다.\n",
    "for _ in range(32):\n",
    "  #jax.random.randint(key, (QUANT_BATCH_SIZE, QUANT_EXAMPLE_LENGTH), 0, 50256): 무작위 정수로 구성된 텐서를 생성합니다. 각 배치는 QUANT_BATCH_SIZE x QUANT_EXAMPLE_LENGTH 크기의 텐서입니다. 각 값은 0에서 50255 사이의 정수입니다 (50256은 GPT-2의 단어 집합 크기입니다).\n",
    "  batch = jax.random.randint(key, (QUANT_BATCH_SIZE, QUANT_EXAMPLE_LENGTH), 0, 50256)\n",
    "  quantization_data.append(batch) #quantization_data.append(batch): 생성된 배치를 양자화 데이터 리스트에 추가합니다.\n",
    "  key, = jax.random.split(key, 1) #랜덤 키를 업데이트하여 다음 배치를 생성할 때 사용할 새로운 키를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "print(type(quantization_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x_pT_fT8Co8"
   },
   "source": [
    "HuggingFace's models don't have quite the right call signature, so we'll make a wrapper which takes (params, inputs) as an argument:\n",
    "주어진 코드는 HuggingFace 모델의 호출 시그니처를 변경하여 JAX 기반의 양자화 함수에 맞게 래퍼를 작성하고, 양자화된 모델 파라미터를 원래의 임베딩 테이블로 대체하는 과정을 포함합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "yddz4OUN8Bvt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing: 0it [00:00, ?it/s]/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:352: UserWarning: Operation gather not supported for quantization\n",
      "  warnings.warn(f'Operation {eqn.primitive.name} not supported for quantization')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 3.28e+04 bytes\n",
      "Current param env size: 2.23e+08 bytes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model(batch, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 작성한 래퍼 함수와 함께 GPT-Q 양자화 함수를 호출하여 양자화된 파라미터를 생성합니다.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\t•\tjax_gptq.quantize 함수 호출: 작성한 래퍼 함수(apply_model), 모델 파라미터(params), 양자화 데이터(quantization_data)를 사용하여 GPT-Q 알고리즘을 통해 모델 파라미터를 양자화합니다.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# •\tquantized_params: 양자화된 파라미터가 저장됩니다.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m quantized_params \u001b[38;5;241m=\u001b[39m jax_gptq\u001b[38;5;241m.\u001b[39mquantize(apply_model, params, quantization_data)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:74\u001b[0m, in \u001b[0;36mquantize\u001b[0;34m(fn, params, inputs, block_size, actorder, damping, use_quantized_activations, use_fp64, use_params_fp32)\u001b[0m\n\u001b[1;32m     72\u001b[0m argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(param_args)))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# `_eval_and_quantize` 함수를 호출하여 양자화된 파라미터를 생성\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m result \u001b[38;5;241m=\u001b[39m _eval_and_quantize(\n\u001b[1;32m     75\u001b[0m     closed_jaxpr\u001b[38;5;241m.\u001b[39mjaxpr,\n\u001b[1;32m     76\u001b[0m     closed_jaxpr\u001b[38;5;241m.\u001b[39mliterals,\n\u001b[1;32m     77\u001b[0m     argnums,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39mparam_args,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39minput_args,\n\u001b[1;32m     80\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m     81\u001b[0m     actorder\u001b[38;5;241m=\u001b[39mactorder,\n\u001b[1;32m     82\u001b[0m     damping\u001b[38;5;241m=\u001b[39mdamping,\n\u001b[1;32m     83\u001b[0m     use_quantized_activations\u001b[38;5;241m=\u001b[39muse_quantized_activations,\n\u001b[1;32m     84\u001b[0m     use_fp64\u001b[38;5;241m=\u001b[39muse_fp64,\n\u001b[1;32m     85\u001b[0m     use_params_fp32\u001b[38;5;241m=\u001b[39muse_params_fp32\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 생성된 양자화된 파라미터를 원래 파라미터 리스트에 다시 할당\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, quantized_param \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:230\u001b[0m, in \u001b[0;36m_eval_and_quantize\u001b[0;34m(jaxpr, consts, argnums, block_size, actorder, damping, use_quantized_activations, use_fp64, use_params_fp32, *args)\u001b[0m\n\u001b[1;32m    226\u001b[0m handler_coro \u001b[38;5;241m=\u001b[39m matmul_handler(all_args)\n\u001b[1;32m    228\u001b[0m w, xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(handler_coro)\n\u001b[0;32m--> 230\u001b[0m quantized_w, quantize_params \u001b[38;5;241m=\u001b[39m gptq(\n\u001b[1;32m    231\u001b[0m     W\u001b[38;5;241m=\u001b[39mw,\n\u001b[1;32m    232\u001b[0m     xs\u001b[38;5;241m=\u001b[39mxs,\n\u001b[1;32m    233\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m    234\u001b[0m     actorder\u001b[38;5;241m=\u001b[39mactorder,\n\u001b[1;32m    235\u001b[0m     damping\u001b[38;5;241m=\u001b[39mdamping,\n\u001b[1;32m    236\u001b[0m     use_fp64\u001b[38;5;241m=\u001b[39muse_fp64\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m quantized_w\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m w\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/gptq.py:203\u001b[0m, in \u001b[0;36mgptq\u001b[0;34m(W, xs, block_size, actorder, damping, use_fp64)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], block_size):\n\u001b[1;32m    202\u001b[0m     end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m block_size\n\u001b[0;32m--> 203\u001b[0m     q_block, errors \u001b[38;5;241m=\u001b[39m _process_block(W, Hinv, start, quantize_params, block_size)\n\u001b[1;32m    204\u001b[0m     update \u001b[38;5;241m=\u001b[39m (Hinv[end:, start:end] \u001b[38;5;241m@\u001b[39m errors)\u001b[38;5;241m.\u001b[39mastype(W\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    205\u001b[0m     W \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mat[end:, :]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m-\u001b[39mupdate)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:327\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 327\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    328\u001b[0m       jit_info, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    329\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    330\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:185\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m pjit_p\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs_flat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/core.py:2834\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2830\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2831\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2832\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2833\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/core.py:420\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    419\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 420\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/core.py:921\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:1635\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1632\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m   1633\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1634\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(\n\u001b[1;32m   1636\u001b[0m     name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[1;32m   1637\u001b[0m     tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, sharding: pxla\u001b[38;5;241m.\u001b[39mshard_args([sharding], [x])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1639\u001b[0m     _get_cpp_global_cache(has_explicit_sharding))(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:1614\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1614\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m _pjit_call_impl_python(\n\u001b[1;32m   1615\u001b[0m       \u001b[38;5;241m*\u001b[39margs, jaxpr\u001b[38;5;241m=\u001b[39mjaxpr, in_shardings\u001b[38;5;241m=\u001b[39min_shardings,\n\u001b[1;32m   1616\u001b[0m       out_shardings\u001b[38;5;241m=\u001b[39mout_shardings, in_layouts\u001b[38;5;241m=\u001b[39min_layouts,\n\u001b[1;32m   1617\u001b[0m       out_layouts\u001b[38;5;241m=\u001b[39mout_layouts, resource_env\u001b[38;5;241m=\u001b[39mresource_env,\n\u001b[1;32m   1618\u001b[0m       donated_invars\u001b[38;5;241m=\u001b[39mdonated_invars, name\u001b[38;5;241m=\u001b[39mname, keep_unused\u001b[38;5;241m=\u001b[39mkeep_unused,\n\u001b[1;32m   1619\u001b[0m       inline\u001b[38;5;241m=\u001b[39minline)\n\u001b[1;32m   1620\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n\u001b[1;32m   1621\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1622\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [], jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m   1623\u001b[0m       jaxpr\u001b[38;5;241m.\u001b[39mconsts, \u001b[38;5;28;01mNone\u001b[39;00m, pgle_profiler)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:1544\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     compile_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfdo_profile\u001b[39m\u001b[38;5;124m'\u001b[39m: fdo_profile}\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;66;03m# TODO(patrios): Do not pass mutable profile session through cached lowering\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;66;03m# chain. Instead we need to move profilers dictionary to pxla module and use\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;66;03m# module as key. Right now we can't do that since there is no way to evict _pjit_lower_cached cache for in PGLE mode.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m compiled \u001b[38;5;241m=\u001b[39m _resolve_and_lower(\n\u001b[1;32m   1537\u001b[0m     args, jaxpr\u001b[38;5;241m=\u001b[39mjaxpr, in_shardings\u001b[38;5;241m=\u001b[39min_shardings,\n\u001b[1;32m   1538\u001b[0m     out_shardings\u001b[38;5;241m=\u001b[39mout_shardings, in_layouts\u001b[38;5;241m=\u001b[39min_layouts,\n\u001b[1;32m   1539\u001b[0m     out_layouts\u001b[38;5;241m=\u001b[39mout_layouts, resource_env\u001b[38;5;241m=\u001b[39mresource_env,\n\u001b[1;32m   1540\u001b[0m     donated_invars\u001b[38;5;241m=\u001b[39mdonated_invars, name\u001b[38;5;241m=\u001b[39mname, keep_unused\u001b[38;5;241m=\u001b[39mkeep_unused,\n\u001b[1;32m   1541\u001b[0m     inline\u001b[38;5;241m=\u001b[39minline, lowering_platforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1542\u001b[0m     lowering_parameters\u001b[38;5;241m=\u001b[39mmlir\u001b[38;5;241m.\u001b[39mLoweringParameters(),\n\u001b[1;32m   1543\u001b[0m     pgle_profiler\u001b[38;5;241m=\u001b[39mpgle_profiler\n\u001b[0;32m-> 1544\u001b[0m )\u001b[38;5;241m.\u001b[39mcompile(compile_options)\n\u001b[1;32m   1546\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2496\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2495\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2496\u001b[0m     executable \u001b[38;5;241m=\u001b[39m UnloadedMeshExecutable\u001b[38;5;241m.\u001b[39mfrom_hlo(\n\u001b[1;32m   2497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hlo, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_args,\n\u001b[1;32m   2498\u001b[0m         compiler_options\u001b[38;5;241m=\u001b[39mcompiler_options)\n\u001b[1;32m   2499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2500\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2995\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2992\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2995\u001b[0m xla_executable \u001b[38;5;241m=\u001b[39m _cached_compilation(\n\u001b[1;32m   2996\u001b[0m     hlo, name, mesh, spmd_lowering,\n\u001b[1;32m   2997\u001b[0m     tuple_args, auto_spmd_lowering, allow_prop_to_inputs,\n\u001b[1;32m   2998\u001b[0m     allow_prop_to_outputs, \u001b[38;5;28mtuple\u001b[39m(host_callbacks), backend, da, pmap_nreps,\n\u001b[1;32m   2999\u001b[0m     compiler_options_keys, compiler_options_values, pgle_profiler)\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[1;32m   3002\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2810\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2802\u001b[0m compile_options \u001b[38;5;241m=\u001b[39m create_compile_options(\n\u001b[1;32m   2803\u001b[0m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[1;32m   2804\u001b[0m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[1;32m   2805\u001b[0m     dev, pmap_nreps, compiler_options)\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2809\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2810\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m compiler\u001b[38;5;241m.\u001b[39mcompile_or_get_cached(\n\u001b[1;32m   2811\u001b[0m       backend, computation, dev, compile_options, host_callbacks,\n\u001b[1;32m   2812\u001b[0m       pgle_profiler)\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/compiler.py:378\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_autotune_config(\n\u001b[1;32m    368\u001b[0m       backend,\n\u001b[1;32m    369\u001b[0m       computation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m       min_device_process_id\n\u001b[1;32m    376\u001b[0m   )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_cache(\n\u001b[1;32m    379\u001b[0m       backend,\n\u001b[1;32m    380\u001b[0m       computation,\n\u001b[1;32m    381\u001b[0m       compile_options,\n\u001b[1;32m    382\u001b[0m       host_callbacks,\n\u001b[1;32m    383\u001b[0m       module_name,\n\u001b[1;32m    384\u001b[0m       cache_key,\n\u001b[1;32m    385\u001b[0m   )\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/compiler.py:608\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    600\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    601\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    606\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    607\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 608\u001b[0m   executable \u001b[38;5;241m=\u001b[39m backend_compile(\n\u001b[1;32m    609\u001b[0m       backend, computation, compile_options, host_callbacks\n\u001b[1;32m    610\u001b[0m   )\n\u001b[1;32m    611\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    612\u001b[0m   _cache_write(\n\u001b[1;32m    613\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    614\u001b[0m   )\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HuggingFace의 모델 호출 시그니처가 JAX의 요구 사항과 맞지 않기 때문에,\n",
    "# (params, inputs) 형식의 인수를 받도록 래퍼를 작성합니다.\n",
    "def apply_model(params, batch):\n",
    "  return model(batch, params=params)\n",
    "# 작성한 래퍼 함수와 함께 GPT-Q 양자화 함수를 호출하여 양자화된 파라미터를 생성합니다.\n",
    "#\t•\tjax_gptq.quantize 함수 호출: 작성한 래퍼 함수(apply_model), 모델 파라미터(params), 양자화 데이터(quantization_data)를 사용하여 GPT-Q 알고리즘을 통해 모델 파라미터를 양자화합니다.\n",
    "# •\tquantized_params: 양자화된 파라미터가 저장됩니다.\n",
    "quantized_params = jax_gptq.quantize(apply_model, params, quantization_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ehblO3I98akJ"
   },
   "outputs": [],
   "source": [
    "# Replace the quantized embedding table with the original one\n",
    "#양자화된 임베딩 테이블을 원래의 임베딩 테이블로 대체합니다.\n",
    "#\t•\t목적: 임베딩 테이블은 입력 토큰을 벡터로 변환하는 중요한 역할을 합니다. 양자화 과정에서 임베딩 테이블이 손상될 수 있으므로, 원래의 임베딩 테이블로 대체하여 정확성을 유지합니다.\n",
    "#   •\tquantized_params['transformer']['wte']['embedding']: 양자화된 파라미터에서 임베딩 테이블을 찾아서 원래의 임베딩 테이블(orig_embedding_table)로 대체합니다.\n",
    "quantized_params['transformer']['wte']['embedding'] = jnp.asarray(orig_embedding_table)\n",
    "quantized_params = jax.device_put(quantized_params, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYiCG5fE9yKT"
   },
   "source": [
    "### Finetuning GPT-2 with Lorax\n",
    "\n",
    "Same as [above](https://colab.research.google.com/drive/18rkULbWqk7mNZDx7Scx-JS3p_s45mgok#scrollTo=HKkhcjx9zJy6&line=3&uniqifier=1), we get the original param structure to tell Lorax how to initialize the LoRA params, then merge the quantized params back in after.\n",
    "\n",
    "이 코드는 LoRA(Low-Rank Adaptation) 기법을 사용하여 양자화된 GPT-2 모델을 미세 조정하는 과정입니다. LoRA는 모델의 일부 파라미터만을 저순위 행렬로 미세 조정함으로써, 메모리 사용량과 계산량을 줄이는 기법입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FKS_dfll93sO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1318360/3777058010.py:38: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(\n"
     ]
    }
   ],
   "source": [
    "# Get pre-quantization param tree (some nodes will just be abstract values)\n",
    "orig_params_or_shapes = jax_gptq.utils.quantized_params_to_shaped_arrays(quantized_params)\n",
    "\n",
    "# Tell Lorax which leaves should be frozen/fully trained/LoRA trained\n",
    "# LoRA 사양을 정의하여 어느 파라미터가 고정되고, 완전 훈련되며, LoRA로 훈련될지 지정합니다.\n",
    "#\t•\tsimple_spec 함수:\n",
    "#   •\torig_params_or_shapes: 원래 파라미터 구조.\n",
    "#   •\tlambda path, arr: 경로에 c_attn 또는 mlp 패턴이 포함된 경우 내적 랭크를 16으로 설정하고, 그렇지 않으면 파라미터를 고정합니다.\n",
    "#   •\ttune_vectors=True: 벡터를 조정하도록 설정합니다.\n",
    "\n",
    "spec = lorax.simple_spec(\n",
    "    orig_params_or_shapes,\n",
    "    lambda path, arr: 16 if any(pattern in path for pattern in ['c_attn', 'mlp']) else lorax.LORA_FREEZE,\n",
    "    tune_vectors=True\n",
    ")\n",
    "\n",
    "# Initialize parameters\n",
    "\"\"\"\n",
    "\t•\t목적: LoRA 파라미터를 초기화합니다.\n",
    "\t•\tjax.random.split 함수: 랜덤 키를 분할하여 초기화 키를 생성합니다.\n",
    "\t•\tinit_lora 함수:\n",
    "\t•\torig_params_or_shapes: 원래 파라미터 구조.\n",
    "\t•\tspec: 정의된 LoRA 사양.\n",
    "\t•\tinit_key: 파라미터 초기화에 사용할 랜덤 키.\n",
    "\n",
    "\"\"\"\n",
    "key, init_key = jax.random.split(key)\n",
    "freeze_params, train_params = lorax.init_lora(\n",
    "    orig_params_or_shapes,\n",
    "    spec,\n",
    "    init_key\n",
    ")\n",
    "# 양자화된 파라미터를 고정된 파라미터 트리에 다시 병합합니다.\n",
    "# Put the quantized params back into the frozen param tree\n",
    "freeze_params = merge_quantized_with_lora(quantized_params, freeze_params)\n",
    "combined_params = freeze_params, train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8bJwqN2Bfqh"
   },
   "source": [
    "Now we can just transform the `apply_model` function and it will use both LoRA and 4-bit quantized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "glARn7Z0BX4g"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\t•\t목적: apply_model 함수를 LoRA와 4-bit 양자화된 파라미터를 모두 사용하도록 변환합니다.\n",
    "\t•\tlorax.lora(apply_model): LoRA 파라미터를 사용하는 함수로 변환합니다.\n",
    "\t•\tjax_gptq.use_quantized: 4-bit 양자화된 파라미터를 사용하는 함수로 변환합니다.\n",
    "\"\"\"\n",
    "\n",
    "quantized_plus_lora_fn = jax_gptq.use_quantized(lorax.lora(apply_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1G-d0yDBn8y"
   },
   "source": [
    "### Training\n",
    "Training isn't actually any different from normal training, since you can just think of `freeze_params` as being a constant argument, but here's a demo for completness.\n",
    "\n",
    "First I'll define a toy corpus which demonstrates Alan's love of cats and Grace's dislike of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I3fdjSioBvDO"
   },
   "outputs": [],
   "source": [
    "# 고양이와 개를 좋아하는 인물에 대한 간단한 텍스트 데이터셋을 만듭니다.\n",
    "CATS = ['lions', 'tigers', 'cheetahs', 'cats', 'ocelots', 'kittens']\n",
    "DOGS = ['wolves', 'dogs', 'coyotes', 'huskies', 'poodles', 'puppies']\n",
    "\n",
    "CAT_LOVER = 'Alan'\n",
    "DOG_LOVER = 'Grace'\n",
    "\n",
    "dataset = []\n",
    "for name, polarity in [(CAT_LOVER, True), (DOG_LOVER, False)]:\n",
    "  liked, disliked = (CATS, DOGS) if polarity else (DOGS, CATS)\n",
    "  for kind in liked:\n",
    "    dataset.append(f'{name}: {kind}? I love them!')\n",
    "    dataset.append(f'{name}: Hey look at those {kind}, that\\'s pretty cool')\n",
    "\n",
    "  for kind in disliked:\n",
    "    dataset.append(f'{name}: {kind}? I hate them!')\n",
    "    dataset.append(f'{name}: Oh no, some {kind}! How scary!')\n",
    "# 텍스트 데이터를 토큰화하고, 최대 길이에 맞춰 패딩합니다.\n",
    "tokenized_data = [jnp.asarray(tokenizer.encode(ex)) for ex in dataset]\n",
    "max_len = max(ex.shape[0] for ex in tokenized_data)\n",
    "# Pad the data to speed up jitting. Not worrying about masking due to laziness.\n",
    "# 패딩을 통해 데이터를 동일한 길이로 맞춥니다. 마스킹은 생략합니다.\n",
    "tokenized_data = [jnp.pad(ex, (0, max_len - ex.shape[0])) for ex in tokenized_data]\n",
    "\"\"\"\n",
    "\t•\t목적: 변환된 모델을 JIT 컴파일하여 성능을 최적화합니다.\n",
    "\t•\tjax.jit: JAX 함수의 JIT 컴파일러를 사용하여 모델을 컴파일합니다.\n",
    "\"\"\"\n",
    "jitted_model = jax.jit(quantized_plus_lora_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NZFLWJgxYqfh"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\t•\t목적: 주어진 프리픽스를 사용하여 모델의 예측을 출력하는 함수를 정의합니다.\n",
    "\t•\ttokenizer.encode: 프리픽스를 토큰화합니다.\n",
    "\t•\tjitted_model(params, tokens[None]): JIT 컴파일된 모델을 사용하여 로짓(logits)을 계산합니다.\n",
    "\t•\tjax.nn.log_softmax: 로짓에 소프트맥스 함수를 적용하여 확률 분포를 계산합니다.\n",
    "\t•\tjax.lax.top_k: 상위 5개의 예측 단어와 그 확률을 찾습니다.\n",
    "\t•\t예측 결과 출력: 프리픽스에 대한 예측 결과를 출력합니다.\n",
    "\n",
    "\"\"\"\n",
    "def make_prediction(params, prefix):\n",
    "  tokens = jnp.asarray(tokenizer.encode(prefix))\n",
    "  logits = jitted_model(params, tokens[None]).logits\n",
    "  \n",
    "  logprobs = jnp.exp(jax.nn.log_softmax(logits[0, -1]))\n",
    "  pred_probs, pred_words = jax.lax.top_k(logprobs, 5)\n",
    "\n",
    "  print(f'Predictions for: \"{prefix}\"')\n",
    "  for i, (word_id, prob) in enumerate(zip(pred_words, pred_probs), 1):\n",
    "    print(f'{i}. {tokenizer.decode([word_id])} - {prob:.2%}')\n",
    "  print()\n",
    "\n",
    "test_examples = [\n",
    "    f'{CAT_LOVER}: jaguars? I',\n",
    "    f'{DOG_LOVER}: jaguars? I'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT7hOBnYS-AC"
   },
   "source": [
    "Let's look at the next word predictions of the unmodified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eew7ihGJTD85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 11:32:30.118895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 11:32:30.192666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 11:32:30.193099: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-23 11:32:31.438438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for: \"Alan: jaguars? I\"\n",
      "1. 'm - 11.25%\n",
      "2.  mean - 8.90%\n",
      "3. 've - 7.17%\n",
      "4.  don - 6.74%\n",
      "5.  thought - 4.55%\n",
      "\n",
      "Predictions for: \"Grace: jaguars? I\"\n",
      "1. 'm - 10.07%\n",
      "2.  don - 7.90%\n",
      "3.  mean - 6.11%\n",
      "4. 've - 6.09%\n",
      "5.  thought - 4.36%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ex in test_examples:\n",
    "  make_prediction(combined_params, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrSL1MgSDXfO"
   },
   "source": [
    "Next we set up a standard training loop. The only difference is that we keep the train/freeze params separate for the optimizer. There's no differences needed for the quantization.\n",
    "\n",
    "I'll just train with a batch size of 1 here since I don't want to bother with masking, but the transformed model function is fully compatible with vmap etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "52QdkmIxDHk-"
   },
   "outputs": [],
   "source": [
    "def loss_fn(train_params, freeze_params, seq):\n",
    "  inputs = seq[:-1]\n",
    "  targets = seq[1:]\n",
    "\n",
    "  combined_params = (freeze_params, train_params)\n",
    "  logits = quantized_plus_lora_fn(combined_params, inputs[None]).logits[0]\n",
    "  logprobs = jax.nn.log_softmax(logits)\n",
    "  losses = -jnp.take_along_axis(logprobs, targets[:, None], axis=-1)\n",
    "  return jnp.mean(losses)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=1e-4, weight_decay=1e-4)\n",
    "opt_state = optimizer.init(combined_params[1])\n",
    "\n",
    "@jax.jit\n",
    "def update_fn(combined_params, opt_state, example):\n",
    "  freeze_params, train_params = combined_params\n",
    "\n",
    "  # The main thing is that we have to split up the params here so that JAX knows what to differentiate with respect to\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(train_params, freeze_params, example)\n",
    "\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params=train_params)\n",
    "  new_train_params = optax.apply_updates(train_params, updates)\n",
    "  return (freeze_params, new_train_params), opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cj2d1xIqFJw3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Loss: 2.581e-01: 100%|██████████| 50/50 [34:29<00:00, 41.38s/it] \n"
     ]
    }
   ],
   "source": [
    "bar = trange(50)\n",
    "for epoch in bar:\n",
    "  key, = jax.random.split(key, 1)\n",
    "  permutation = jax.random.permutation(key, jnp.arange(len(dataset)))\n",
    "  total_loss = 0\n",
    "  for index in permutation:\n",
    "    example = tokenized_data[index]\n",
    "    combined_params, opt_state, loss = update_fn(combined_params, opt_state, example)\n",
    "    total_loss += loss\n",
    "  bar.set_description(f'Epoch {epoch} - Loss: {total_loss / len(tokenized_data):.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMFZwE8qeSUl"
   },
   "source": [
    "The trained LoRA parameters give us a model which predicts that Alan will love jaguars, and Grace will hate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GIgThnapFQS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for: \"Alan: jaguars? I\"\n",
      "1.  love - 83.57%\n",
      "2.  hate - 16.42%\n",
      "3.  LOVE - 0.00%\n",
      "4.  like - 0.00%\n",
      "5.  want - 0.00%\n",
      "\n",
      "\n",
      "Predictions for: \"Grace: jaguars? I\"\n",
      "1.  hate - 62.58%\n",
      "2.  love - 37.39%\n",
      "3.  LOVE - 0.01%\n",
      "4. 'll - 0.01%\n",
      "5.  Hate - 0.01%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in test_examples:\n",
    "  make_prediction(combined_params, example)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92W8jCjQeZ9J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Y6JeyF45yd_"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
