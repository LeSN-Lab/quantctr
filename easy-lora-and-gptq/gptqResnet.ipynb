{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y6JeyF45yd_"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "75-T_R0Ms9qD"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "# 아래 코드는 원하는 GPU 번호만 쓰도록 설정하는 코드\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import lorax\n",
    "import jax_gptq\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "gpu = jax.devices('gpu')[0]\n",
    "cpu = jax.devices('cpu')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Djyo_reAs26R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/daehun/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#/home/quantctr/jax-resnet/jax_resnet를 sys.path에 추가\n",
    "sys.path.append('/home/quantctr/jax-resnet/jax_resnet')\n",
    "# ResNet 모델 로드\n",
    "from jax_resnet.pretrained import pretrained_resnet\n",
    "\n",
    "# ResNet 크기 선택 (예: 50)\n",
    "size = 50\n",
    "model_cls, params = pretrained_resnet(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "92W8jCjQeZ9J"
   },
   "outputs": [],
   "source": [
    "\n",
    "params = jax.device_put(params, gpu)\n",
    "\n",
    "# 모델 적용 함수 정의\n",
    "def apply_model(params, batch):\n",
    "    return model_cls().apply(params, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = model_cls().apply(params,\n",
    "                  jnp.ones((32, 224, 224, 3)),  # ImageNet sized inputs.\n",
    "                  mutable=False)  # Ensure `batch_stats` aren't updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AI2/anaconda3/envs/12jax/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 기존 코드에서 정의된 모델, 훈련 상태 생성 함수 등은 그대로 사용\n",
    "\n",
    "def create_jax_datasets(val_dataset, batch_size):\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    def numpy_collate(batch):\n",
    "        if isinstance(batch[0], np.ndarray):\n",
    "            return np.stack(batch)\n",
    "        elif isinstance(batch[0], (tuple,list)):\n",
    "            transposed = zip(*batch)\n",
    "            return [numpy_collate(samples) for samples in transposed]\n",
    "        else:\n",
    "            return np.array(batch)\n",
    "\n",
    "    def to_jax_batch(batch):\n",
    "        images, labels = batch\n",
    "        # Transpose images to (batch_size, height, width, channels)\n",
    "        images = jnp.array(images.numpy()).transpose(0, 2, 3, 1)\n",
    "\n",
    "        return {\n",
    "            'image': jnp.array(images),\n",
    "            'label': jnp.array(labels.numpy())\n",
    "        }\n",
    "\n",
    "    jax_val_dataset = map(to_jax_batch, val_loader)\n",
    "    \n",
    "    return jax_val_dataset\n",
    "\n",
    "# 데이터셋 준비\n",
    "valdir = os.path.join('/home/quantctr/easy-lora-and-gptq','val')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    valdir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "# JAX 데이터셋 생성\n",
    "batch_size = 64\n",
    "jax_val_dataset = create_jax_datasets(val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:589: UserWarning: Currently only quantizing convs with 1x..x1 kernels are supported\n",
      "  warnings.warn('Currently only quantizing convs with 1x..x1 kernels are supported')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 1.27e+09 bytes\n",
      "Current param env size: 3.87e+04 bytes\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:589: UserWarning: Currently only quantizing convs with 1x..x1 kernels are supported\n",
      "  warnings.warn('Currently only quantizing convs with 1x..x1 kernels are supported')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current env size: 3.39e+09 bytes\n",
      "Current param env size: 1.50e+05 bytes\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n",
      "padding ((0, 0), (0, 0))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# params = jax.device_put(params, gpu)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(type((quantization_data[0])))\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m quantized_params \u001b[38;5;241m=\u001b[39m jax_gptq\u001b[38;5;241m.\u001b[39mquantize(apply_model, params, quantization_data)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:74\u001b[0m, in \u001b[0;36mquantize\u001b[0;34m(fn, params, inputs, block_size, actorder, damping, use_quantized_activations, use_fp64, use_params_fp32)\u001b[0m\n\u001b[1;32m     72\u001b[0m argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(param_args)))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# `_eval_and_quantize` 함수를 호출하여 양자화된 파라미터를 생성\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m result \u001b[38;5;241m=\u001b[39m _eval_and_quantize(\n\u001b[1;32m     75\u001b[0m     closed_jaxpr\u001b[38;5;241m.\u001b[39mjaxpr,\n\u001b[1;32m     76\u001b[0m     closed_jaxpr\u001b[38;5;241m.\u001b[39mliterals,\n\u001b[1;32m     77\u001b[0m     argnums,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39mparam_args,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39minput_args,\n\u001b[1;32m     80\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m     81\u001b[0m     actorder\u001b[38;5;241m=\u001b[39mactorder,\n\u001b[1;32m     82\u001b[0m     damping\u001b[38;5;241m=\u001b[39mdamping,\n\u001b[1;32m     83\u001b[0m     use_quantized_activations\u001b[38;5;241m=\u001b[39muse_quantized_activations,\n\u001b[1;32m     84\u001b[0m     use_fp64\u001b[38;5;241m=\u001b[39muse_fp64,\n\u001b[1;32m     85\u001b[0m     use_params_fp32\u001b[38;5;241m=\u001b[39muse_params_fp32\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 생성된 양자화된 파라미터를 원래 파라미터 리스트에 다시 할당\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, quantized_param \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax_gptq/quantize_interpreter.py:266\u001b[0m, in \u001b[0;36m_eval_and_quantize\u001b[0;34m(jaxpr, consts, argnums, block_size, actorder, damping, use_quantized_activations, use_fp64, use_params_fp32, *args)\u001b[0m\n\u001b[1;32m    259\u001b[0m gpu_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    260\u001b[0m     matmul_w_arg\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argname \u001b[38;5;241m==\u001b[39m quantize_argname \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     env[argname]\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m argname \u001b[38;5;129;01min\u001b[39;00m matmul_eqn\u001b[38;5;241m.\u001b[39minvars\n\u001b[1;32m    264\u001b[0m ]\n\u001b[1;32m    265\u001b[0m gpu_args \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_put(gpu_args, gpu)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m do_eval(\u001b[38;5;241m*\u001b[39mgpu_args)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree_size_bytes(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e8\u001b[39m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# This should offload stuff like the final logits to the CPU\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     cpu_results \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_put(results, cpu)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:327\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 327\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    328\u001b[0m       jit_info, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    329\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    330\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:185\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m pjit_p\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs_flat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/core.py:2834\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2830\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2831\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2832\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2833\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/core.py:420\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    419\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 420\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/core.py:921\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:1635\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1632\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m   1633\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1634\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(\n\u001b[1;32m   1636\u001b[0m     name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[1;32m   1637\u001b[0m     tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, sharding: pxla\u001b[38;5;241m.\u001b[39mshard_args([sharding], [x])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1639\u001b[0m     _get_cpp_global_cache(has_explicit_sharding))(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:1614\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1614\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m _pjit_call_impl_python(\n\u001b[1;32m   1615\u001b[0m       \u001b[38;5;241m*\u001b[39margs, jaxpr\u001b[38;5;241m=\u001b[39mjaxpr, in_shardings\u001b[38;5;241m=\u001b[39min_shardings,\n\u001b[1;32m   1616\u001b[0m       out_shardings\u001b[38;5;241m=\u001b[39mout_shardings, in_layouts\u001b[38;5;241m=\u001b[39min_layouts,\n\u001b[1;32m   1617\u001b[0m       out_layouts\u001b[38;5;241m=\u001b[39mout_layouts, resource_env\u001b[38;5;241m=\u001b[39mresource_env,\n\u001b[1;32m   1618\u001b[0m       donated_invars\u001b[38;5;241m=\u001b[39mdonated_invars, name\u001b[38;5;241m=\u001b[39mname, keep_unused\u001b[38;5;241m=\u001b[39mkeep_unused,\n\u001b[1;32m   1619\u001b[0m       inline\u001b[38;5;241m=\u001b[39minline)\n\u001b[1;32m   1620\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n\u001b[1;32m   1621\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1622\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [], jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m   1623\u001b[0m       jaxpr\u001b[38;5;241m.\u001b[39mconsts, \u001b[38;5;28;01mNone\u001b[39;00m, pgle_profiler)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/pjit.py:1544\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     compile_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfdo_profile\u001b[39m\u001b[38;5;124m'\u001b[39m: fdo_profile}\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;66;03m# TODO(patrios): Do not pass mutable profile session through cached lowering\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;66;03m# chain. Instead we need to move profilers dictionary to pxla module and use\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;66;03m# module as key. Right now we can't do that since there is no way to evict _pjit_lower_cached cache for in PGLE mode.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m compiled \u001b[38;5;241m=\u001b[39m _resolve_and_lower(\n\u001b[1;32m   1537\u001b[0m     args, jaxpr\u001b[38;5;241m=\u001b[39mjaxpr, in_shardings\u001b[38;5;241m=\u001b[39min_shardings,\n\u001b[1;32m   1538\u001b[0m     out_shardings\u001b[38;5;241m=\u001b[39mout_shardings, in_layouts\u001b[38;5;241m=\u001b[39min_layouts,\n\u001b[1;32m   1539\u001b[0m     out_layouts\u001b[38;5;241m=\u001b[39mout_layouts, resource_env\u001b[38;5;241m=\u001b[39mresource_env,\n\u001b[1;32m   1540\u001b[0m     donated_invars\u001b[38;5;241m=\u001b[39mdonated_invars, name\u001b[38;5;241m=\u001b[39mname, keep_unused\u001b[38;5;241m=\u001b[39mkeep_unused,\n\u001b[1;32m   1541\u001b[0m     inline\u001b[38;5;241m=\u001b[39minline, lowering_platforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1542\u001b[0m     lowering_parameters\u001b[38;5;241m=\u001b[39mmlir\u001b[38;5;241m.\u001b[39mLoweringParameters(),\n\u001b[1;32m   1543\u001b[0m     pgle_profiler\u001b[38;5;241m=\u001b[39mpgle_profiler\n\u001b[0;32m-> 1544\u001b[0m )\u001b[38;5;241m.\u001b[39mcompile(compile_options)\n\u001b[1;32m   1546\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2496\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2495\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2496\u001b[0m     executable \u001b[38;5;241m=\u001b[39m UnloadedMeshExecutable\u001b[38;5;241m.\u001b[39mfrom_hlo(\n\u001b[1;32m   2497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hlo, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_args,\n\u001b[1;32m   2498\u001b[0m         compiler_options\u001b[38;5;241m=\u001b[39mcompiler_options)\n\u001b[1;32m   2499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2500\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2995\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2992\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2995\u001b[0m xla_executable \u001b[38;5;241m=\u001b[39m _cached_compilation(\n\u001b[1;32m   2996\u001b[0m     hlo, name, mesh, spmd_lowering,\n\u001b[1;32m   2997\u001b[0m     tuple_args, auto_spmd_lowering, allow_prop_to_inputs,\n\u001b[1;32m   2998\u001b[0m     allow_prop_to_outputs, \u001b[38;5;28mtuple\u001b[39m(host_callbacks), backend, da, pmap_nreps,\n\u001b[1;32m   2999\u001b[0m     compiler_options_keys, compiler_options_values, pgle_profiler)\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[1;32m   3002\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2810\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2802\u001b[0m compile_options \u001b[38;5;241m=\u001b[39m create_compile_options(\n\u001b[1;32m   2803\u001b[0m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[1;32m   2804\u001b[0m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[1;32m   2805\u001b[0m     dev, pmap_nreps, compiler_options)\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2809\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2810\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m compiler\u001b[38;5;241m.\u001b[39mcompile_or_get_cached(\n\u001b[1;32m   2811\u001b[0m       backend, computation, dev, compile_options, host_callbacks,\n\u001b[1;32m   2812\u001b[0m       pgle_profiler)\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/compiler.py:378\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_autotune_config(\n\u001b[1;32m    368\u001b[0m       backend,\n\u001b[1;32m    369\u001b[0m       computation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m       min_device_process_id\n\u001b[1;32m    376\u001b[0m   )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_cache(\n\u001b[1;32m    379\u001b[0m       backend,\n\u001b[1;32m    380\u001b[0m       computation,\n\u001b[1;32m    381\u001b[0m       compile_options,\n\u001b[1;32m    382\u001b[0m       host_callbacks,\n\u001b[1;32m    383\u001b[0m       module_name,\n\u001b[1;32m    384\u001b[0m       cache_key,\n\u001b[1;32m    385\u001b[0m   )\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/compiler.py:608\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    600\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    601\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    606\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    607\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 608\u001b[0m   executable \u001b[38;5;241m=\u001b[39m backend_compile(\n\u001b[1;32m    609\u001b[0m       backend, computation, compile_options, host_callbacks\n\u001b[1;32m    610\u001b[0m   )\n\u001b[1;32m    611\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    612\u001b[0m   _cache_write(\n\u001b[1;32m    613\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    614\u001b[0m   )\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "QUANT_BATCH_SIZE = 4 #\t•\tQUANT_BATCH_SIZE: 양자화를 위해 사용할 배치 크기입니다. 여기서는 4로 설정되어 있습니다.\n",
    "#양자화 예제의 길이입니다. 각 예제는 64개의 토큰으로 구성됩니다. 이 값을 더 크게 설정할 수 있지만, Colab에서 메모리 충돌을 방지하기 위해 작은 값으로 설정되었습니다\n",
    "QUANT_EXAMPLE_LENGTH = 64 # I'd recommend making this bigger, but needs to be small to not crash colab\n",
    "\n",
    "quantization_data = []\n",
    "key = jax.random.PRNGKey(0) #JAX의 랜덤 키를 초기화합니다. 랜덤 키는 재현 가능한 무작위 값을 생성하는 데 사용됩니다.\n",
    "for batch in jax_val_dataset:\n",
    "    # 배치 데이터 추출\n",
    "    images = batch['image']\n",
    "    \n",
    "    labels = batch['label']\n",
    "    \n",
    "    # GPU로 배치 이동\n",
    "    images = jax.device_put(images, gpu)\n",
    "    quantization_data.append(images) #quantization_data.append(batch): 생성된 배치를 양자화 데이터 리스트에 추가합니다.\n",
    "    if len(quantization_data) > 32:\n",
    "      break\n",
    "\n",
    "# params = jax.device_put(params, gpu)\n",
    "# print(type((quantization_data[0])))\n",
    "quantized_params = jax_gptq.quantize(apply_model, params, quantization_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 224, 224, 3)\n",
      "Batch 1 processed, Batch Accuracy: 0.8125, Total samples: 64\n",
      "(64, 224, 224, 3)\n",
      "Batch 2 processed, Batch Accuracy: 0.9219, Total samples: 128\n",
      "(64, 224, 224, 3)\n",
      "Batch 3 processed, Batch Accuracy: 0.7969, Total samples: 192\n",
      "(64, 224, 224, 3)\n",
      "Batch 4 processed, Batch Accuracy: 0.8438, Total samples: 256\n",
      "(64, 224, 224, 3)\n",
      "Batch 5 processed, Batch Accuracy: 0.8594, Total samples: 320\n",
      "(64, 224, 224, 3)\n",
      "Batch 6 processed, Batch Accuracy: 0.8125, Total samples: 384\n",
      "(64, 224, 224, 3)\n",
      "Batch 7 processed, Batch Accuracy: 0.9531, Total samples: 448\n",
      "(64, 224, 224, 3)\n",
      "Batch 8 processed, Batch Accuracy: 0.9219, Total samples: 512\n",
      "(64, 224, 224, 3)\n",
      "Batch 9 processed, Batch Accuracy: 0.9531, Total samples: 576\n",
      "(64, 224, 224, 3)\n",
      "Batch 10 processed, Batch Accuracy: 0.8750, Total samples: 640\n",
      "\n",
      "Inference completed\n",
      "Overall Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# TensorFlow 데이터셋을 NumPy 배열로 변환\n",
    "jax_val_dataset\n",
    "\n",
    "batch_count = 0\n",
    "total_processed = 0\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch in jax_val_dataset:\n",
    "    # 배치 데이터 추출\n",
    "    images = batch['image']\n",
    "    \n",
    "    labels = batch['label']\n",
    "    \n",
    "    # GPU로 배치 이동\n",
    "    images = jax.device_put(images, gpu)\n",
    "    \n",
    "    print(images.shape)\n",
    "    # print(len(params[\"params\"], len()))\n",
    "    # 모델 적용\n",
    "    outputs = apply_model(params, images)\n",
    "    \n",
    "     # 예측 클래스 계산\n",
    "    predicted_classes = jnp.argmax(outputs, axis=1)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    correct_predictions = jnp.sum(predicted_classes == labels)\n",
    "    total_correct += correct_predictions\n",
    "    total_samples += labels.shape[0]\n",
    "    \n",
    "    # 배치 정확도 계산\n",
    "    batch_accuracy = correct_predictions / labels.shape[0]\n",
    "    \n",
    "    batch_count += 1\n",
    "    print(f\"Batch {batch_count} processed, Batch Accuracy: {batch_accuracy:.4f}, Total samples: {total_samples}\")\n",
    "\n",
    "    #옵션: 특정 수의 배치 후에 중단\n",
    "    if batch_count >= 10:\n",
    "        break\n",
    "\n",
    "# 전체 정확도 계산\n",
    "overall_accuracy = total_correct / total_samples\n",
    "print(f\"\\nInference completed\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 224, 224, 3)\n",
      "Batch 1 processed, Batch Accuracy: 0.4844, Total samples: 64\n",
      "(64, 224, 224, 3)\n",
      "Batch 2 processed, Batch Accuracy: 0.7500, Total samples: 128\n",
      "(64, 224, 224, 3)\n",
      "Batch 3 processed, Batch Accuracy: 0.7188, Total samples: 192\n",
      "(64, 224, 224, 3)\n",
      "Batch 4 processed, Batch Accuracy: 0.7500, Total samples: 256\n",
      "(64, 224, 224, 3)\n",
      "Batch 5 processed, Batch Accuracy: 0.5938, Total samples: 320\n",
      "(64, 224, 224, 3)\n",
      "Batch 6 processed, Batch Accuracy: 0.6406, Total samples: 384\n",
      "(64, 224, 224, 3)\n",
      "Batch 7 processed, Batch Accuracy: 0.5938, Total samples: 448\n",
      "(64, 224, 224, 3)\n",
      "Batch 8 processed, Batch Accuracy: 0.9688, Total samples: 512\n",
      "(64, 224, 224, 3)\n",
      "Batch 9 processed, Batch Accuracy: 0.9062, Total samples: 576\n",
      "(64, 224, 224, 3)\n",
      "Batch 10 processed, Batch Accuracy: 0.7812, Total samples: 640\n",
      "\n",
      "Inference completed\n",
      "Overall Accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "quantized_params = jax.device_put(quantized_params, gpu)\n",
    "quantized_fn = jax_gptq.use_quantized(apply_model)\n",
    "jitted_model = jax.jit(quantized_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# TensorFlow 데이터셋을 NumPy 배열로 변환\n",
    "jax_val_dataset\n",
    "\n",
    "batch_count = 0\n",
    "total_processed = 0\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch in jax_val_dataset:\n",
    "    # 배치 데이터 추출\n",
    "    images = batch['image']\n",
    "    \n",
    "    labels = batch['label']\n",
    "    \n",
    "    # GPU로 배치 이동\n",
    "    images = jax.device_put(images, gpu)\n",
    "    \n",
    "    print(images.shape)\n",
    "    # print(len(params[\"params\"], len()))\n",
    "    # 모델 적용\n",
    "    outputs = jitted_model(quantized_params, images)\n",
    "    \n",
    "     # 예측 클래스 계산\n",
    "    predicted_classes = jnp.argmax(outputs, axis=1)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    correct_predictions = jnp.sum(predicted_classes == labels)\n",
    "    total_correct += correct_predictions\n",
    "    total_samples += labels.shape[0]\n",
    "    \n",
    "    # 배치 정확도 계산\n",
    "    batch_accuracy = correct_predictions / labels.shape[0]\n",
    "    \n",
    "    batch_count += 1\n",
    "    print(f\"Batch {batch_count} processed, Batch Accuracy: {batch_accuracy:.4f}, Total samples: {total_samples}\")\n",
    "\n",
    "    #옵션: 특정 수의 배치 후에 중단\n",
    "    if batch_count >= 10:\n",
    "        break\n",
    "\n",
    "# 전체 정확도 계산\n",
    "overall_accuracy = total_correct / total_samples\n",
    "print(f\"\\nInference completed\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original params size: 102.44 MB\n",
      "Quantized params size: 53.01 MB\n",
      "Compression ratio: 1.93x\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def get_params_size(params):\n",
    "    total_size = 0\n",
    "    for param in jax.tree_util.tree_leaves(params):\n",
    "        total_size += param.size * param.dtype.itemsize\n",
    "    return total_size\n",
    "\n",
    "original_size = get_params_size(params)\n",
    "quantized_size = get_params_size(quantized_params)\n",
    "\n",
    "print(f\"Original params size: {original_size / 1e6:.2f} MB\")\n",
    "print(f\"Quantized params size: {quantized_size / 1e6:.2f} MB\")\n",
    "print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model average inference time: 200.82 ms\n",
      "Quantized model average inference time: 218.03 ms\n",
      "Speedup: 0.92x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def time_inference(model_fn, params, input_data, num_runs=100):\n",
    "    # 워밍업 실행\n",
    "    for _ in range(5):\n",
    "        _ = model_fn(params, input_data)\n",
    "    \n",
    "    # 메인 타이밍 루프\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = model_fn(params, input_data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time\n",
    "\n",
    "# 샘플 입력 데이터 준비\n",
    "sample_batch = next(iter(jax_val_dataset))\n",
    "sample_images = jax.device_put(sample_batch['image'], gpu)\n",
    "# 원본 모델 함수\n",
    "original_fn = jax.jit(apply_model)\n",
    "\n",
    "# 양자화된 모델 함수\n",
    "quantized_fn = jax.jit(jax_gptq.use_quantized(apply_model))\n",
    "\n",
    "# 원본 모델 추론 시간 측정\n",
    "original_time = time_inference(original_fn, params, sample_images)\n",
    "\n",
    "# 양자화된 모델 추론 시간 측정\n",
    "quantized_time = time_inference(quantized_fn, quantized_params, sample_images)\n",
    "\n",
    "print(f\"Original model average inference time: {original_time*1000:.2f} ms\")\n",
    "print(f\"Quantized model average inference time: {quantized_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {original_time/quantized_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Y6JeyF45yd_"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
