{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 21:27:53.920280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-24 21:27:53.946746: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-24 21:27:53.946784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 21:27:54.942132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from models import VisionTransformer\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import jax_gptq\n",
    "from jax.core import ShapedArray\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def get_datasets(batch_size):\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "        'cifar10',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    \n",
    "    def preprocess_data(image, label):\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        label = tf.one_hot(label, 10)\n",
    "        return image, label\n",
    "\n",
    "    test_ds = ds_test.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(params, dataset):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataset, desc=\"Computing accuracy\", unit=\"batch\"):\n",
    "        images = jnp.array(images.numpy())\n",
    "        labels = jnp.array(labels.numpy())\n",
    "        \n",
    "        outputs, intermediates = apply_model(params, images)\n",
    "        print(\"Model output shape:\", outputs.shape)\n",
    "        print(\"Sample output:\", outputs[0])\n",
    "        print(\"Predicted class:\", jnp.argmax(outputs[0]))\n",
    "        print(\"True class:\", jnp.argmax(labels[0]))\n",
    "        print(\"Unique predicted classes:\", np.unique(jnp.argmax(outputs, axis=1)))\n",
    "        predictions = jnp.argmax(outputs, axis=1)\n",
    "        true_labels = jnp.argmax(labels, axis=1)\n",
    "        correct_predictions += jnp.sum(predictions == true_labels)\n",
    "        total_predictions += labels.shape[0]\n",
    "        \n",
    "        # 각 배치의 정확도를 출력\n",
    "        batch_accuracy = jnp.sum(predictions == true_labels) / labels.shape[0]\n",
    "        print(f\"Batch accuracy: {batch_accuracy:.4f}\")\n",
    "    \n",
    "    return correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    model_name = 'ViT-B_16'\n",
    "    patch_size = dict(size=(16, 16))\n",
    "    transformer = dict(num_layers=12, mlp_dim=3072, num_heads=12, dropout_rate=0.1, attention_dropout_rate=0.1)\n",
    "    model = VisionTransformer(num_classes=10, patch_size=patch_size, transformer=transformer, hidden_size=768, representation_size=None)\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(params, x, train=False):\n",
    "    return model.apply(\n",
    "        {'params': params},\n",
    "        x,\n",
    "        train=train,\n",
    "        capture_intermediates=True,\n",
    "        mutable=['intermediates']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_params(flat_params):\n",
    "    new_params = {}\n",
    "    for key, value in flat_params.items():\n",
    "        parts = key.split('/')\n",
    "        current = new_params\n",
    "        for part in parts[:-1]:\n",
    "            if part not in current:\n",
    "                current[part] = {}\n",
    "            current = current[part]\n",
    "        current[parts[-1]] = value\n",
    "    return new_params\n",
    "\n",
    "def quantized_params_to_shaped_arrays(tree):\n",
    "    def _shape_from_param(x):\n",
    "        if isinstance(x, jax_gptq.QuantizedMatrix):\n",
    "            return ShapedArray(jax_gptq.quant_matrix_shape(x), x.zero.dtype)\n",
    "        return x\n",
    "    return jax.tree_map(_shape_from_param, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화된 모델의 정확도 평가 (옵션)\n",
    "def compute_accuracy(params, dataset):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in dataset:\n",
    "        images = batch['image']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = apply_model(params, images)\n",
    "        predictions = jnp.argmax(outputs, axis=-1)\n",
    "        correct_predictions += jnp.sum(predictions == labels)\n",
    "        total_predictions += labels.shape[0]\n",
    "    \n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "print(\"Computing quantized model accuracy...\")\n",
    "quantized_accuracy = compute_accuracy(quantized_params, jax_val_dataset)\n",
    "print(f\"Quantized model accuracy: {quantized_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/deepops/temp/easy-lora-and-gptq/checkpoint/imagenet21k_ViT-B_16.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model, model_name \u001b[38;5;241m=\u001b[39m my_model()\n\u001b[1;32m      2\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/deepops/temp/easy-lora-and-gptq/checkpoint/imagenet21k_ViT-B_16.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     flat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(f, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m restructure_params(flat_params)\n",
      "File \u001b[0;32m/home/AI2/anaconda3/envs/12jax/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/deepops/temp/easy-lora-and-gptq/checkpoint/imagenet21k_ViT-B_16.npz'"
     ]
    }
   ],
   "source": [
    "    model, model_name = my_model()\n",
    "    checkpoint_path = '/data/deepops/temp/easy-lora-and-gptq/checkpoint/imagenet21k_ViT-B_16.npz'\n",
    "\n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        flat_params = dict(np.load(f, allow_pickle=True))\n",
    "    \n",
    "    params = restructure_params(flat_params)\n",
    "\n",
    "    # 'head' 조정 부분\n",
    "    if 'head' in params:\n",
    "        output_features = params['head']['kernel'].shape[1]\n",
    "        if output_features != 10:\n",
    "            params['head']['kernel'] = params['head']['kernel'][:, :10]\n",
    "            params['head']['bias'] = params['head']['bias'][:10]\n",
    "\n",
    "    # 모델 초기화 및 파라미터 병합\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    init_variables = model.init(\n",
    "        rng,\n",
    "        jnp.ones((1, 224, 224, 3), jnp.float32),\n",
    "        train=False\n",
    "    )\n",
    "\n",
    "    init_params = init_variables['params']\n",
    "    for key in init_params.keys():\n",
    "        if key not in params:\n",
    "            params[key] = init_params[key]\n",
    "        elif isinstance(params[key], dict) and isinstance(init_params[key], dict):\n",
    "            for subkey in init_params[key].keys():\n",
    "                if subkey not in params[key]:\n",
    "                    params[key][subkey] = init_params[key][subkey]\n",
    "\n",
    "    test_ds = get_datasets(batch_size)\n",
    "    \n",
    "    # head 레이어 재초기화\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    head_params = model.init(init_rng, jnp.ones((1, 224, 224, 3), jnp.float32), train=False)['params']['head']\n",
    "    params['head'] = head_params\n",
    "\n",
    "    for images, labels in test_ds.take(1):\n",
    "        print(\"Sample label:\", labels[0].numpy())\n",
    "        images = jnp.array(images.numpy())\n",
    "        labels = jnp.array(labels.numpy())\n",
    "        outputs, variables = apply_model(params, images)\n",
    "        \n",
    "    print(\"Computing original model accuracy...\")\n",
    "    original_accuracy = compute_accuracy(params, test_ds)\n",
    "    print(f\"Original model accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "    QUANT_BATCH_SIZE = 1\n",
    "    QUANT_EXAMPLE_LENGTH = 224\n",
    "    quantization_data = []\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    for _ in tqdm(range(32), desc=\"Preparing quantization data\", unit=\"batch\"):\n",
    "        batch = jax.random.uniform(key, (QUANT_BATCH_SIZE, QUANT_EXAMPLE_LENGTH, QUANT_EXAMPLE_LENGTH, 3))\n",
    "        quantization_data.append(batch)\n",
    "        key, = jax.random.split(key, 1)\n",
    "    print(\"Quantization data preparation completed.\")\n",
    "\n",
    "    quantized_params = jax_gptq.quantize(apply_model, params, quantization_data)\n",
    "\n",
    "    gpu = jax.devices('gpu')[0]\n",
    "    quantized_params = jax.device_put(quantized_params, gpu)\n",
    "\n",
    "    print(\"Computing quantized model accuracy...\")\n",
    "    def apply_quantized_model(params, x):\n",
    "        shaped_params = quantized_params_to_shaped_arrays(params)\n",
    "        return jax_gptq.use_quantized(apply_model)(shaped_params, x)\n",
    "\n",
    "    # JIT 컴파일을 수행합니다.\n",
    "    jitted_quantized_model = jax.jit(apply_quantized_model)\n",
    "\n",
    "    quantized_accuracy = compute_accuracy(quantized_params, jitted_quantized_model, test_ds)\n",
    "    print(f\"Quantized model accuracy: {quantized_accuracy:.4f}\")\n",
    "\n",
    "    accuracy_drop = original_accuracy - quantized_accuracy\n",
    "    print(f\"Accuracy drop: {accuracy_drop:.4f}\")\n",
    "    print(f\"Relative accuracy drop: {(accuracy_drop / original_accuracy) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "12jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
